{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88326f00-28bb-412b-8ea7-674847c95c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training samples: 162770, Validation samples: 19867\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    root_dir = 'D:/celeba/img_align_celeba/img_align_celeba'\n",
    "    csv_path = 'D:/celeba/list_attr_celeba.csv'\n",
    "    partition_csv_path = 'D:/celeba/list_eval_partition.csv'\n",
    "    num_classes = 40\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    grad_accum_steps = 2\n",
    "    grad_clip = 1.0\n",
    "    max_epochs = 10\n",
    "    pretrained_weights = models.ResNet50_Weights.IMAGENET1K_V2\n",
    "    early_stop_patience=5\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    attribute_names = [\n",
    "        '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
    "        'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
    "        'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
    "        'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
    "        'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
    "        'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks',\n",
    "        'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n",
    "        'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n",
    "    ]\n",
    "\n",
    "# Dataset class with header handling\n",
    "class CelebADataset(Dataset):\n",
    "    def __init__(self, root_dir, csv_path, partition_csv_path, split, transform=None):\n",
    "        # Load attributes\n",
    "        self.df = pd.read_csv(csv_path).replace(-1, 0)\n",
    "        \n",
    "        # Load partition data with header\n",
    "        partition_df = pd.read_csv(partition_csv_path)\n",
    "        partition_df['partition'] = partition_df['partition'].astype(int)\n",
    "        \n",
    "        # Merge datasets\n",
    "        self.df = self.df.merge(partition_df, on='image_id', how='inner')\n",
    "        \n",
    "        # Convert split name to code\n",
    "        split_codes = {'train': 0, 'valid': 1, 'validation': 1, 'test': 2}\n",
    "        split = split.lower()\n",
    "        if split not in split_codes:\n",
    "            raise ValueError(f\"Invalid split: {split}. Use train/valid/test\")\n",
    "            \n",
    "        split_code = split_codes[split]\n",
    "        self.df = self.df[self.df['partition'] == split_code].copy()\n",
    "        \n",
    "        # Validate dataset\n",
    "        if len(self.df) == 0:\n",
    "            raise ValueError(f\"No samples found for {split} partition\")\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.labels = self.df.drop(['image_id', 'partition'], axis=1).values.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.df.iloc[idx]['image_id'])\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {str(e)}\")\n",
    "            image = torch.zeros(3, 224, 224)\n",
    "        return image, self.labels[idx]\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(Config.mean, Config.std)\n",
    "])\n",
    "\n",
    "eval_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(Config.mean, Config.std)\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "try:\n",
    "    train_set = CelebADataset(\n",
    "        root_dir=Config.root_dir,\n",
    "        csv_path=Config.csv_path,\n",
    "        partition_csv_path=Config.partition_csv_path,\n",
    "        split='train',\n",
    "        transform=train_transform\n",
    "    )\n",
    "\n",
    "    val_set = CelebADataset(\n",
    "        root_dir=Config.root_dir,\n",
    "        csv_path=Config.csv_path,\n",
    "        partition_csv_path=Config.partition_csv_path,\n",
    "        split='valid',\n",
    "        transform=eval_transform\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error creating dataset: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=Config.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_set)}, Validation samples: {len(val_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4aed1ba4-2e6d-4fc7-a6da-6d5b72a24d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(train_set, Config.batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, Config.batch_size, pin_memory=True)\n",
    "# Optimized Model Class\n",
    "class AttributeClassifier:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = models.resnet50(weights=config.pretrained_weights)\n",
    "        \n",
    "        # Freeze layers\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.model.layer3.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in self.model.layer4.parameters():\n",
    "            param.requires_grad = True\n",
    "            \n",
    "        # Modified head\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, config.num_classes)\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=config.lr,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def train(self, train_loader, val_loader):\n",
    "        best_f1 = 0\n",
    "        epochs_no_improve = 0\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            # Training phase\n",
    "            self.model.train()\n",
    "            epoch_train_loss = 0.0\n",
    "            \n",
    "            for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs = inputs.to(self.device, non_blocking=True)\n",
    "                labels = labels.to(self.device, non_blocking=True)\n",
    "                \n",
    "                # Mixed precision forward pass\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = self.model(inputs)\n",
    "                    loss = self.criterion(outputs, labels) / self.config.grad_accum_steps\n",
    "                \n",
    "                # Backward pass with gradient scaling\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient accumulation steps\n",
    "                if (batch_idx + 1) % self.config.grad_accum_steps == 0:\n",
    "                    # Unscale before clipping\n",
    "                    scaler.unscale_(self.optimizer)\n",
    "                    \n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.config.grad_clip\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    scaler.step(self.optimizer)\n",
    "                    scaler.update()\n",
    "                    self.optimizer.zero_grad(set_to_none=True)\n",
    "                \n",
    "                epoch_train_loss += loss.item() * self.config.grad_accum_steps\n",
    "            \n",
    "            # Calculate average training loss\n",
    "            avg_train_loss = epoch_train_loss / len(train_loader)\n",
    "            \n",
    "            # Validation phase\n",
    "            avg_val_loss, avg_f1 = self._validate(val_loader)\n",
    "            \n",
    "            # Early stopping logic\n",
    "            if avg_f1 > best_f1:\n",
    "                best_f1 = avg_f1\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(self.model.state_dict(), 'best_model.pth')\n",
    "                print(f\"Epoch {epoch+1}/{self.config.max_epochs}\")\n",
    "                print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"Val F1: {avg_f1:.4f}* (Best)\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"Epoch {epoch+1}/{self.config.max_epochs}\")\n",
    "                print(f\"Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "                print(f\"Val F1: {avg_f1:.4f} (No improvement {epochs_no_improve}/{self.config.early_stop_patience})\")\n",
    "            \n",
    "            if epochs_no_improve >= self.config.early_stop_patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n",
    "                break\n",
    "    \n",
    "        # Load best model weights\n",
    "        self.model.load_state_dict(torch.load('best_model.pth'))\n",
    "        print(\"Training complete. Loaded best model weights.\")\n",
    "\n",
    "    def _validate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_f1 = 0.0\n",
    "        \n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device).float()\n",
    "                outputs = self.model(inputs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                val_loss += self.criterion(outputs, labels).item()\n",
    "                \n",
    "                # Calculate F1\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                total_f1 += self._calculate_f1(preds, labels)\n",
    "    \n",
    "        return val_loss/len(val_loader), total_f1/len(val_loader)\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_f1(self, preds, labels):\n",
    "            # Convert boolean masks to float for calculations\n",
    "        preds = preds.bool()\n",
    "        labels = labels.bool()\n",
    "            \n",
    "        tp = (preds & labels).sum(0, dtype=torch.float32)\n",
    "        fp = (preds & ~labels).sum(0, dtype=torch.float32)\n",
    "        fn = (~preds & labels).sum(0, dtype=torch.float32)\n",
    "            \n",
    "        precision = tp / (tp + fp + 1e-9)\n",
    "        recall = tp / (tp + fn + 1e-9)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        return f1.mean().item()\n",
    "        \n",
    "    def test(self, test_loader, top_ks=[5, 10, 20, 30]):\n",
    "        self.model.eval()\n",
    "        results = {\n",
    "            'strict': 0.0,\n",
    "            'mean': 0.0,\n",
    "            'top_acc': {k: 0.0 for k in top_ks},\n",
    "            'per_attribute': {}\n",
    "        }\n",
    "        attr_correct = torch.zeros(self.config.num_classes).to(self.device)\n",
    "        total_samples = 0\n",
    "        \n",
    "        with torch.no_grad(), torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                batch_size = inputs.size(0)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > 0.5).float()\n",
    "                \n",
    "                # Per-image metrics\n",
    "                results['strict'] += (preds == labels).all(dim=1).sum().item()\n",
    "                results['mean'] += (preds == labels).float().mean(dim=1).sum().item()\n",
    "                \n",
    "                # Per-attribute metrics\n",
    "                attr_correct += (preds == labels).sum(dim=0)\n",
    "                \n",
    "                # Top-k calculations\n",
    "                for k in top_ks:\n",
    "                    topk_probs, topk_indices = torch.topk(probs, k, dim=1)\n",
    "                    correct = torch.gather(labels, 1, topk_indices).sum(dim=1)\n",
    "                    results['top_acc'][k] += (correct.float() / k).sum().item()\n",
    "                \n",
    "                total_samples += batch_size\n",
    "    \n",
    "        # Calculate final metrics\n",
    "        results['strict'] /= total_samples\n",
    "        results['mean'] /= total_samples\n",
    "        for k in top_ks:\n",
    "            results['top_acc'][k] /= total_samples\n",
    "        \n",
    "        # Per-attribute accuracies\n",
    "        per_attr_acc = (attr_correct / total_samples).cpu().numpy()\n",
    "        for idx, acc in enumerate(per_attr_acc):\n",
    "            results['per_attribute'][self.config.attribute_names[idx]] = acc\n",
    "    \n",
    "        # Print results\n",
    "        print(\"\\n=== Test Results ===\")\n",
    "        print(f\"Strict Accuracy: {results['strict']:.4f}\")\n",
    "        print(f\"Mean Accuracy: {results['mean']:.4f}\")\n",
    "        for k in sorted(top_ks):\n",
    "            print(f\"Top-{k} Accuracy: {results['top_acc'][k]:.4f}\")\n",
    "        \n",
    "        # Print per-attribute accuracies\n",
    "        # Inside the test() method, replace the print block with:\n",
    "        print(\"\\nPer-Attribute Accuracy Ranking:\")\n",
    "        print(\"-\" * 65)\n",
    "        print(f\"{'Rank':<5}{'Attribute':<30}{'Accuracy':<10} | {'Rank':<5}{'Attribute':<30}{'Accuracy':<10}\")\n",
    "        print(\"-\" * 65)\n",
    "        \n",
    "        sorted_attrs = sorted(results['per_attribute'].items(), key=lambda x: x[1], reverse=True)\n",
    "        for i in range(0, len(sorted_attrs), 2):\n",
    "            line = \"\"\n",
    "            # First column\n",
    "            if i < len(sorted_attrs):\n",
    "                name, acc = sorted_attrs[i]\n",
    "                line += f\"{i+1:<5}{name:<30}{acc:.4f}    \"\n",
    "            else:\n",
    "                line += \" \" * 45\n",
    "                \n",
    "            # Second column\n",
    "            line += \"| \"\n",
    "            if i+1 < len(sorted_attrs):\n",
    "                name, acc = sorted_attrs[i+1]\n",
    "                line += f\"{i+2:<5}{name:<30}{acc:.4f}\"\n",
    "            \n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8decce-6a8a-4afd-9d7d-0849458a351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "# Training Execution\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    classifier = AttributeClassifier(config)\n",
    "    classifier.train(train_loader, val_loader)\n",
    "    print(\"Training completed. Best model saved as 'celeba_teacher.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "183b75cd-1fcf-4183-849f-3bc737748b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Results ===\n",
      "Strict Accuracy: 0.0250\n",
      "Mean Accuracy: 0.9134\n",
      "Top-5 Accuracy: 0.9115\n",
      "Top-10 Accuracy: 0.7423\n",
      "Top-20 Accuracy: 0.4588\n",
      "Top-30 Accuracy: 0.3083\n",
      "\n",
      "Per-Attribute Accuracy Ranking:\n",
      "-----------------------------------------------------------------\n",
      "Rank Attribute                     Accuracy   | Rank Attribute                     Accuracy  \n",
      "-----------------------------------------------------------------\n",
      "1    Eyeglasses                    0.9970    | 2    Wearing_Hat                   0.9916\n",
      "3    Bald                          0.9904    | 4    Male                          0.9852\n",
      "5    Gray_Hair                     0.9830    | 6    Sideburns                     0.9791\n",
      "7    Goatee                        0.9755    | 8    Pale_Skin                     0.9716\n",
      "9    Mustache                      0.9704    | 10   No_Beard                      0.9644\n",
      "11   Blurry                        0.9638    | 12   Double_Chin                   0.9619\n",
      "13   Blond_Hair                    0.9614    | 14   Bangs                         0.9606\n",
      "15   Wearing_Necktie               0.9566    | 16   Rosy_Cheeks                   0.9515\n",
      "17   5_o_Clock_Shadow              0.9491    | 18   Chubby                        0.9491\n",
      "19   Mouth_Slightly_Open           0.9403    | 20   Receding_Hairline             0.9381\n",
      "21   Wearing_Lipstick              0.9341    | 22   Smiling                       0.9311\n",
      "23   Bushy_Eyebrows                0.9262    | 24   Heavy_Makeup                  0.9179\n",
      "25   Wearing_Earrings              0.9104    | 26   Black_Hair                    0.9030\n",
      "27   Young                         0.8848    | 28   Brown_Hair                    0.8816\n",
      "29   High_Cheekbones               0.8770    | 30   Narrow_Eyes                   0.8729\n",
      "31   Wearing_Necklace              0.8662    | 32   Bags_Under_Eyes               0.8540\n",
      "33   Wavy_Hair                     0.8500    | 34   Straight_Hair                 0.8447\n",
      "35   Arched_Eyebrows               0.8381    | 36   Attractive                    0.8303\n",
      "37   Big_Nose                      0.8191    | 38   Pointy_Nose                   0.7693\n",
      "39   Oval_Face                     0.7611    | 40   Big_Lips                      0.7228\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m classifier\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m strict_acc, mean_acc \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mtest(test_loader)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config()\n",
    "    \n",
    "    # Create test dataset\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(config.mean, config.std)\n",
    "    ])\n",
    "    \n",
    "    test_set = CelebADataset(\n",
    "        root_dir=config.root_dir,\n",
    "        csv_path=config.csv_path,\n",
    "        partition_csv_path=config.partition_csv_path,\n",
    "        split='test',\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_set,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize and load model\n",
    "    classifier = AttributeClassifier(config)\n",
    "    classifier.model.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    # Run evaluation\n",
    "    strict_acc, mean_acc = classifier.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ed5b2a-2144-44e7-8f63-9402c235c111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44737d3-4406-44f5-97cd-2c86e544c3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604c58c-1851-4dc6-aab3-2c3a5247cc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
