{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"40a168c622264fd8a9a18bd5d5f5a2d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_341184c514654651b88f485680e4da55","IPY_MODEL_14f6b872ff714a2d920af59fbfad387f","IPY_MODEL_219c0bd19ca443d082c5ce36a7372378"],"layout":"IPY_MODEL_ae4bb571332b4dafba47a388d89536fe"}},"341184c514654651b88f485680e4da55":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef9ce1ae63964de3bd75bd622825983e","placeholder":"​","style":"IPY_MODEL_0236a42237be4a1795a9f0052fe7fc32","value":"Extracting: 100%"}},"14f6b872ff714a2d920af59fbfad387f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_066f0b2e15c9410b93178d34bf2b1f37","max":202600,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a38156c92ca2450fb53609041ad9ee62","value":202600}},"219c0bd19ca443d082c5ce36a7372378":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68d7e85170e64255881da92f4c186379","placeholder":"​","style":"IPY_MODEL_5579767c442a436cb33982e84d94bd20","value":" 202600/202600 [00:22&lt;00:00, 9773.59it/s]"}},"ae4bb571332b4dafba47a388d89536fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef9ce1ae63964de3bd75bd622825983e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0236a42237be4a1795a9f0052fe7fc32":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"066f0b2e15c9410b93178d34bf2b1f37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a38156c92ca2450fb53609041ad9ee62":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68d7e85170e64255881da92f4c186379":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5579767c442a436cb33982e84d94bd20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import zipfile\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from PIL import Image\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","# 1. Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# 2. Extract CelebA ZIP (update your path)\n","zip_path = '/content/drive/MyDrive/img_align_celeba.zip'  # Change to your path\n","extract_path = '/content/celeba'\n","\n","if not os.path.exists(extract_path):\n","    os.makedirs(extract_path, exist_ok=True)\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        for file in tqdm(zip_ref.namelist(), desc='Extracting'):\n","            zip_ref.extract(file, extract_path)\n","\n","# 3. Dataset Class with Splitting Capability\n","class CelebADataset(Dataset):\n","    def __init__(self, root_dir, split='train', transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","                                      delim_whitespace=True, header=1)\n","        self.filenames = self.attributes.index.tolist()\n","        self.attributes = (self.attributes + 1) // 2  # Convert -1/1 to 0/1\n","\n","        # Load the official split file\n","        split_file = os.path.join(root_dir, 'list_eval_partition.txt')\n","        split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n","\n","        # Filter based on requested split\n","        if split == 'train':\n","            self.filenames = [f for f in self.filenames if split_info.loc[f].partition == 0]\n","        elif split == 'valid':\n","            self.filenames = [f for f in self.filenames if split_info.loc[f].partition == 1]\n","        elif split == 'test':\n","            self.filenames = [f for f in self.filenames if split_info.loc[f].partition == 2]\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.root_dir, 'img_align_celeba', self.filenames[idx])\n","        try:\n","            img = Image.open(img_path).convert('RGB')\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            img = torch.zeros(3, 128, 128)  # Return a blank image on failure\n","\n","        attrs = self.attributes.loc[self.filenames[idx]].values.astype('float32')\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, torch.from_numpy(attrs)\n","\n","# 4. Define Transforms\n","transform = transforms.Compose([\n","    transforms.Resize(128),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","# 5. Create DataLoaders\n","train_dataset = CelebADataset(extract_path, split='train', transform=transform)\n","valid_dataset = CelebADataset(extract_path, split='valid', transform=transform)\n","test_dataset = CelebADataset(extract_path, split='test', transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Verify\n","images, attrs = next(iter(train_loader))\n","print(f\"Train batch: {images.shape}, {attrs.shape}\")\n","print(f\"Train samples: {len(train_loader.dataset)}, Valid samples: {len(valid_loader.dataset)}, Test samples: {len(test_loader.dataset)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":341,"referenced_widgets":["40a168c622264fd8a9a18bd5d5f5a2d3","341184c514654651b88f485680e4da55","14f6b872ff714a2d920af59fbfad387f","219c0bd19ca443d082c5ce36a7372378","ae4bb571332b4dafba47a388d89536fe","ef9ce1ae63964de3bd75bd622825983e","0236a42237be4a1795a9f0052fe7fc32","066f0b2e15c9410b93178d34bf2b1f37","a38156c92ca2450fb53609041ad9ee62","68d7e85170e64255881da92f4c186379","5579767c442a436cb33982e84d94bd20"]},"id":"JpmQW0liF5YE","outputId":"47aa7dc3-5124-44b9-8781-686b964df3fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"display_data","data":{"text/plain":["Extracting:   0%|          | 0/202600 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a168c622264fd8a9a18bd5d5f5a2d3"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["<ipython-input-1-aee666df0010>:29: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","<ipython-input-1-aee666df0010>:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n","<ipython-input-1-aee666df0010>:29: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","<ipython-input-1-aee666df0010>:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n","<ipython-input-1-aee666df0010>:29: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","<ipython-input-1-aee666df0010>:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n"]},{"output_type":"stream","name":"stdout","text":["Train batch: torch.Size([64, 3, 156, 128]), torch.Size([64, 40])\n","Train samples: 162770, Valid samples: 19867, Test samples: 19962\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y08EzRyKFC5m","outputId":"4ed0b087-1c16-41f8-f95b-6dea53779ea6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading teacher model...\n","Initializing student model...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 118MB/s]\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","<ipython-input-8-930bf041bc5e>:116: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["\n","Starting knowledge distillation training...\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1:   0%|          | 0/2544 [00:00<?, ?it/s]<ipython-input-8-930bf041bc5e>:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Epoch 1: 100%|██████████| 2544/2544 [08:21<00:00,  5.07it/s, loss=0.2638, kl=0.0572, bce=0.4704]\n","<ipython-input-8-930bf041bc5e>:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/30\n","Train Loss: 0.2638 (KL: 0.0572, BCE: 0.4704)\n","Val Loss: 0.2580 (KL: 0.0593, BCE: 0.4567)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 2544/2544 [07:32<00:00,  5.62it/s, loss=0.2557, kl=0.0589, bce=0.4524]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/30\n","Train Loss: 0.2557 (KL: 0.0589, BCE: 0.4524)\n","Val Loss: 0.2558 (KL: 0.0569, BCE: 0.4548)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 2544/2544 [07:27<00:00,  5.68it/s, loss=0.2526, kl=0.0595, bce=0.4456]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/30\n","Train Loss: 0.2526 (KL: 0.0595, BCE: 0.4456)\n","Val Loss: 0.2558 (KL: 0.0619, BCE: 0.4496)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 2544/2544 [07:31<00:00,  5.63it/s, loss=0.2497, kl=0.0603, bce=0.4392]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/30\n","Train Loss: 0.2497 (KL: 0.0603, BCE: 0.4392)\n","Val Loss: 0.2555 (KL: 0.0612, BCE: 0.4499)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|██████████| 2544/2544 [07:28<00:00,  5.67it/s, loss=0.2465, kl=0.0612, bce=0.4319]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/30\n","Train Loss: 0.2465 (KL: 0.0612, BCE: 0.4319)\n","Val Loss: 0.2570 (KL: 0.0575, BCE: 0.4566)\n","Current LR: 3.00e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 2544/2544 [07:29<00:00,  5.66it/s, loss=0.2427, kl=0.0622, bce=0.4231]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/30\n","Train Loss: 0.2427 (KL: 0.0622, BCE: 0.4231)\n","Val Loss: 0.2586 (KL: 0.0632, BCE: 0.4540)\n","Current LR: 3.00e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|██████████| 2544/2544 [07:34<00:00,  5.60it/s, loss=0.2385, kl=0.0634, bce=0.4136]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/30\n","Train Loss: 0.2385 (KL: 0.0634, BCE: 0.4136)\n","Val Loss: 0.2636 (KL: 0.0627, BCE: 0.4645)\n","Current LR: 3.00e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|██████████| 2544/2544 [07:34<00:00,  5.60it/s, loss=0.2345, kl=0.0646, bce=0.4045]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/30\n","Train Loss: 0.2345 (KL: 0.0646, BCE: 0.4045)\n","Val Loss: 0.2675 (KL: 0.0631, BCE: 0.4718)\n","Current LR: 1.50e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|██████████| 2544/2544 [07:36<00:00,  5.57it/s, loss=0.2259, kl=0.0671, bce=0.3846]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/30\n","Train Loss: 0.2259 (KL: 0.0671, BCE: 0.3846)\n","Val Loss: 0.2774 (KL: 0.0650, BCE: 0.4897)\n","Current LR: 1.50e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|██████████| 2544/2544 [07:30<00:00,  5.64it/s, loss=0.2210, kl=0.0686, bce=0.3734]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10/30\n","Train Loss: 0.2210 (KL: 0.0686, BCE: 0.3734)\n","Val Loss: 0.2877 (KL: 0.0616, BCE: 0.5138)\n","Current LR: 1.50e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11: 100%|██████████| 2544/2544 [07:35<00:00,  5.58it/s, loss=0.2179, kl=0.0696, bce=0.3661]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11/30\n","Train Loss: 0.2179 (KL: 0.0696, BCE: 0.3661)\n","Val Loss: 0.2961 (KL: 0.0672, BCE: 0.5250)\n","Current LR: 1.50e-04\n","Early stopping after 11 epochs\n","\n","Evaluating student model on test set:\n","\n","=== Evaluation Results ===\n","Strict Accuracy: 0.0132\n","Mean Accuracy: 0.8977\n","Top-5 Accuracy: 0.9106\n","Top-10 Accuracy: 0.7392\n","Top-20 Accuracy: 0.4549\n","\n","Per-Attribute Metrics (Top 10 by F1 score):\n","-------------------------------------------------------------------------------------\n","Attribute                Accuracy  Precision Recall    F1        Support   \n","-------------------------------------------------------------------------------------\n","No_Beard                 0.9562    0.9579    0.9923    0.9748       17041.0\n","Eyeglasses               0.9966    0.9788    0.9682    0.9735        1289.0\n","Male                     0.9782    0.9599    0.9846    0.9721        7715.0\n","Wearing_Lipstick         0.9428    0.9209    0.9742    0.9468       10418.0\n","Mouth_Slightly_Open      0.9354    0.9345    0.9350    0.9348        9883.0\n","Smiling                  0.9268    0.9326    0.9202    0.9264        9987.0\n","Young                    0.8625    0.8554    0.9849    0.9156       15114.0\n","Heavy_Makeup             0.9133    0.8741    0.9181    0.8956        8084.0\n","Wearing_Hat              0.9904    0.8657    0.9142    0.8893         839.0\n","Bangs                    0.9589    0.8582    0.8820    0.8699        3109.0\n","\n","Saved final student model\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","from tqdm import tqdm\n","import numpy as np\n","\n","# Configuration\n","class Config:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    num_classes = 40\n","    batch_size = 128\n","    lr = 3e-4\n","    max_epochs = 30\n","    temperature = 1.0  # Start with simpler temperature\n","    alpha = 0.5  # Balanced weight between teacher and ground truth\n","    patience = 7\n","    grad_clip = 1.0\n","    grad_accum_steps = 2\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","    attribute_names = [\n","        '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n","        'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n","        'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n","        'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n","        'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n","        'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks',\n","        'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n","        'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n","    ]\n","\n","# Teacher Model (ResNet50)\n","class TeacherModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n","\n","        # Freeze early layers\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","        for param in self.backbone.layer3.parameters():\n","            param.requires_grad = True\n","        for param in self.backbone.layer4.parameters():\n","            param.requires_grad = True\n","\n","        # Classifier head\n","        self.backbone.fc = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(self.backbone.fc.in_features, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, num_classes))\n","\n","    def forward(self, x):\n","        return self.backbone(x)\n","\n","# Student Model (ResNet18)\n","class StudentModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","        self.backbone.fc = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.Linear(self.backbone.fc.in_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_classes))\n","\n","    def forward(self, x):\n","        return self.backbone(x)\n","\n","# Proper Distillation Loss for Multi-Label Classification\n","class DistillationLoss(nn.Module):\n","    def __init__(self, temperature, alpha):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.bce_loss = nn.BCEWithLogitsLoss()\n","\n","    def forward(self, student_logits, teacher_logits, targets):\n","        # Teacher probabilities with temperature\n","        teacher_probs = torch.sigmoid(teacher_logits / self.temperature)\n","        student_probs = torch.sigmoid(student_logits / self.temperature)\n","\n","        # BCE loss with original logits (not temperature-scaled)\n","        bce_loss = self.bce_loss(student_logits, targets)\n","\n","        # KL divergence between teacher and student probabilities\n","        kl_loss = (teacher_probs * (torch.log(teacher_probs + 1e-10) -\n","                  torch.log(student_probs + 1e-10))).mean()\n","\n","        # Combined loss\n","        total_loss = (self.alpha * kl_loss +\n","                     (1 - self.alpha) * bce_loss)\n","\n","        return total_loss, {'kl_loss': kl_loss.item(), 'bce_loss': bce_loss.item()}\n","\n","# Training Function\n","def train_with_distillation(student, teacher, train_loader, val_loader):\n","    optimizer = optim.AdamW(\n","        student.parameters(),\n","        lr=Config.lr,\n","        weight_decay=1e-4\n","    )\n","\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode='min',\n","        factor=0.5,\n","        patience=3,\n","        verbose=True\n","    )\n","\n","    criterion = DistillationLoss(Config.temperature, Config.alpha)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","\n","    for epoch in range(Config.max_epochs):\n","        student.train()\n","        train_loss = 0.0\n","        loss_components = {'kl_loss': 0.0, 'bce_loss': 0.0}\n","\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","\n","        for step, (images, labels) in enumerate(pbar):\n","            images = images.to(Config.device, non_blocking=True)\n","            labels = labels.to(Config.device, non_blocking=True)\n","\n","            with torch.cuda.amp.autocast():\n","                student_logits = student(images)\n","                with torch.no_grad():\n","                    teacher_logits = teacher(images)\n","                loss, comp = criterion(student_logits, teacher_logits, labels)\n","                loss = loss / Config.grad_accum_steps\n","\n","            scaler.scale(loss).backward()\n","\n","            if (step + 1) % Config.grad_accum_steps == 0:\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(student.parameters(), Config.grad_clip)\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad(set_to_none=True)\n","\n","            train_loss += loss.item() * Config.grad_accum_steps\n","            for k in loss_components:\n","                loss_components[k] += comp[k]\n","            pbar.set_postfix({\n","                'loss': f\"{train_loss/(step+1):.4f}\",\n","                'kl': f\"{loss_components['kl_loss']/(step+1):.4f}\",\n","                'bce': f\"{loss_components['bce_loss']/(step+1):.4f}\"\n","            })\n","\n","        # Validation\n","        val_loss = 0.0\n","        val_components = {'kl_loss': 0.0, 'bce_loss': 0.0}\n","        student.eval()\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images = images.to(Config.device)\n","                labels = labels.to(Config.device)\n","\n","                with torch.cuda.amp.autocast():\n","                    student_logits = student(images)\n","                    teacher_logits = teacher(images)\n","                    loss, comp = criterion(student_logits, teacher_logits, labels)\n","                    val_loss += loss.item()\n","                    for k in val_components:\n","                        val_components[k] += comp[k]\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","        avg_val_loss = val_loss / len(val_loader)\n","\n","        scheduler.step(avg_val_loss)\n","\n","        print(f\"\\nEpoch {epoch+1}/{Config.max_epochs}\")\n","        print(f\"Train Loss: {avg_train_loss:.4f} (KL: {loss_components['kl_loss']/len(train_loader):.4f}, BCE: {loss_components['bce_loss']/len(train_loader):.4f})\")\n","        print(f\"Val Loss: {avg_val_loss:.4f} (KL: {val_components['kl_loss']/len(val_loader):.4f}, BCE: {val_components['bce_loss']/len(val_loader):.4f})\")\n","        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","            torch.save({\n","                'model': student.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'epoch': epoch,\n","                'best_val_loss': best_val_loss\n","            }, 'best_student_model.pth')\n","            print(\"Saved best student model\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= Config.patience:\n","                print(f\"Early stopping after {epoch+1} epochs\")\n","                break\n","\n","    checkpoint = torch.load('best_student_model.pth')\n","    student.load_state_dict(checkpoint['model'])\n","    return student\n","\n","# Evaluation Function\n","def evaluate_model(model, loader, attribute_names=None):\n","    model.eval()\n","    results = {\n","        'strict_acc': 0.0,\n","        'mean_acc': 0.0,\n","        'top_k_acc': {5: 0.0, 10: 0.0, 20: 0.0},\n","        'per_attribute_acc': np.zeros(Config.num_classes),\n","        'confusion_matrix': np.zeros((2, 2, Config.num_classes))\n","    }\n","\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images = images.to(Config.device)\n","            labels = labels.to(Config.device).float()\n","            batch_size = images.size(0)\n","\n","            outputs = model(images)\n","            probs = torch.sigmoid(outputs)\n","            preds = (probs > 0.5).float()\n","\n","            results['strict_acc'] += (preds == labels).all(dim=1).sum().item()\n","            results['mean_acc'] += (preds == labels).float().mean(dim=1).sum().item()\n","\n","            for k in results['top_k_acc'].keys():\n","                _, topk_indices = torch.topk(probs, k, dim=1)\n","                correct = torch.gather(labels, 1, topk_indices).sum(dim=1)\n","                results['top_k_acc'][k] += (correct.float() / k).sum().item()\n","\n","            for attr in range(Config.num_classes):\n","                attr_pred = preds[:, attr]\n","                attr_label = labels[:, attr]\n","                results['per_attribute_acc'][attr] += (attr_pred == attr_label).sum().item()\n","                results['confusion_matrix'][0, 0, attr] += ((attr_pred == 1) & (attr_label == 1)).sum().item()\n","                results['confusion_matrix'][0, 1, attr] += ((attr_pred == 1) & (attr_label == 0)).sum().item()\n","                results['confusion_matrix'][1, 0, attr] += ((attr_pred == 0) & (attr_label == 1)).sum().item()\n","                results['confusion_matrix'][1, 1, attr] += ((attr_pred == 0) & (attr_label == 0)).sum().item()\n","\n","            total_samples += batch_size\n","\n","    results['strict_acc'] /= total_samples\n","    results['mean_acc'] /= total_samples\n","    for k in results['top_k_acc']:\n","        results['top_k_acc'][k] /= total_samples\n","    results['per_attribute_acc'] /= total_samples\n","\n","    attribute_metrics = {}\n","    for attr in range(Config.num_classes):\n","        tp = results['confusion_matrix'][0, 0, attr]\n","        fp = results['confusion_matrix'][0, 1, attr]\n","        fn = results['confusion_matrix'][1, 0, attr]\n","        tn = results['confusion_matrix'][1, 1, attr]\n","\n","        precision = tp / (tp + fp + 1e-10)\n","        recall = tp / (tp + fn + 1e-10)\n","        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n","\n","        attr_name = attribute_names[attr] if attribute_names else f\"Attr {attr}\"\n","        attribute_metrics[attr_name] = {\n","            'accuracy': (tp + tn) / (tp + tn + fp + fn),\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'support': tp + fn\n","        }\n","\n","    print(\"\\n=== Evaluation Results ===\")\n","    print(f\"Strict Accuracy: {results['strict_acc']:.4f}\")\n","    print(f\"Mean Accuracy: {results['mean_acc']:.4f}\")\n","    for k, acc in sorted(results['top_k_acc'].items()):\n","        print(f\"Top-{k} Accuracy: {acc:.4f}\")\n","\n","    if attribute_names:\n","        print(\"\\nPer-Attribute Metrics (Top 10 by F1 score):\")\n","        print(\"-\" * 85)\n","        print(f\"{'Attribute':<25}{'Accuracy':<10}{'Precision':<10}{'Recall':<10}{'F1':<10}{'Support':<10}\")\n","        print(\"-\" * 85)\n","\n","        sorted_attrs = sorted(attribute_metrics.items(),\n","                            key=lambda x: x[1]['f1'],\n","                            reverse=True)[:10]\n","\n","        for name, metrics in sorted_attrs:\n","            print(f\"{name:<25}{metrics['accuracy']:.4f}    {metrics['precision']:.4f}    {metrics['recall']:.4f}    {metrics['f1']:.4f}    {metrics['support']:>10}\")\n","\n","    return results, attribute_metrics\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Initialize data loaders (you need to define these)\n","    # train_loader = ...\n","    # valid_loader = ...\n","    # test_loader = ...\n","\n","    # Load teacher model\n","    print(\"Loading teacher model...\")\n","    teacher = TeacherModel(num_classes=Config.num_classes)\n","    teacher_checkpoint = torch.load('/content/drive/MyDrive/best_model.pth', map_location=Config.device)\n","\n","    # Handle potential DataParallel wrapper\n","    if all(k.startswith('module.') for k in teacher_checkpoint.keys()):\n","        from collections import OrderedDict\n","        new_state_dict = OrderedDict()\n","        for k, v in teacher_checkpoint.items():\n","            name = k[7:]  # remove 'module.' prefix\n","            new_state_dict[name] = v\n","        teacher_checkpoint = new_state_dict\n","\n","    teacher.load_state_dict(teacher_checkpoint, strict=False)\n","    teacher = teacher.to(Config.device)\n","    teacher.eval()\n","\n","    # Create student model\n","    print(\"Initializing student model...\")\n","    student = StudentModel(num_classes=Config.num_classes)\n","    student = student.to(Config.device)\n","\n","    # Train with distillation\n","    print(\"\\nStarting knowledge distillation training...\")\n","    trained_student = train_with_distillation(student, teacher, train_loader, valid_loader)\n","\n","    # Evaluate on test set\n","    print(\"\\nEvaluating student model on test set:\")\n","    test_results, attribute_metrics = evaluate_model(trained_student, test_loader, Config.attribute_names)\n","\n","    # Save final student model\n","    torch.save(trained_student.state_dict(), 'final_student_model.pth')\n","    print(\"\\nSaved final student model\")"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import models\n","from tqdm import tqdm\n","import numpy as np\n","from collections import OrderedDict\n","\n","# Configuration\n","class Config:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    num_classes = 40\n","    batch_size = 128\n","    lr = 3e-4\n","    max_epochs = 30\n","    temperature = 3.0  # Increased temperature for softer probabilities\n","    alpha = 0.3       # Weight for attention transfer\n","    beta = 0.3        # Weight for distillation loss\n","    patience = 7\n","    grad_clip = 1.0\n","    grad_accum_steps = 2\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","    attribute_names = [\n","        '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n","        'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n","        'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n","        'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n","        'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n","        'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks',\n","        'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n","        'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n","    ]\n","\n","# Teacher Model\n","class TeacherModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.base_model = models.resnet50(weights=None)\n","\n","        # Replace classifier head\n","        self.base_model.fc = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(self.base_model.fc.in_features, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, num_classes))\n","\n","        # Attention maps storage\n","        self.attention_maps = {}\n","        self._register_hooks()\n","\n","    def _register_hooks(self):\n","        def get_activation(name):\n","            def hook(model, input, output):\n","                # Store attention maps after ReLU activation\n","                self.attention_maps[name] = output.detach()\n","            return hook\n","\n","        # Register hooks for intermediate layers\n","        self.base_model.layer1.register_forward_hook(get_activation('layer1'))\n","        self.base_model.layer2.register_forward_hook(get_activation('layer2'))\n","        self.base_model.layer3.register_forward_hook(get_activation('layer3'))\n","        self.base_model.layer4.register_forward_hook(get_activation('layer4'))\n","\n","    def forward(self, x):\n","        self.attention_maps.clear()\n","        return self.base_model(x)\n","\n","# Student Model\n","class StudentModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","\n","        # Replace classifier head\n","        self.base_model.fc = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.Linear(self.base_model.fc.in_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_classes))\n","\n","        self.attention_maps = {}\n","        self._register_hooks()\n","\n","    def _register_hooks(self):\n","        def get_activation(name):\n","            def hook(model, input, output):\n","                self.attention_maps[name] = output\n","            return hook\n","\n","        self.base_model.layer1.register_forward_hook(get_activation('layer1'))\n","        self.base_model.layer2.register_forward_hook(get_activation('layer2'))\n","        self.base_model.layer3.register_forward_hook(get_activation('layer3'))\n","        self.base_model.layer4.register_forward_hook(get_activation('layer4'))\n","\n","    def forward(self, x):\n","        self.attention_maps.clear()\n","        return self.base_model(x)\n","\n","# Corrected Attention Transfer Loss\n","class AttentionTransferLoss(nn.Module):\n","    def __init__(self, temperature=3.0, alpha=0.3, beta=0.3):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha  # Weight for attention transfer\n","        self.beta = beta    # Weight for distillation loss\n","        self.bce_loss = nn.BCEWithLogitsLoss()\n","\n","    def _attention_transfer(self, teacher_feat, student_feat):\n","        \"\"\"Compute attention transfer loss between teacher and student features\"\"\"\n","        # Get attention maps using squared activations\n","        teacher_attention = torch.sum(teacher_feat.pow(2), dim=1)\n","        student_attention = torch.sum(student_feat.pow(2), dim=1)\n","\n","        # Normalize attention maps\n","        teacher_attention = F.normalize(teacher_attention.view(teacher_attention.size(0), -1), p=2, dim=1)\n","        student_attention = F.normalize(student_attention.view(student_attention.size(0), -1), p=2, dim=1)\n","\n","        return F.mse_loss(student_attention, teacher_attention)\n","\n","    def forward(self, student_logits, teacher_logits, student_maps, teacher_maps, targets):\n","        # Attention transfer loss\n","        attn_loss = 0\n","        layer_weights = {'layer1': 0.1, 'layer2': 0.2, 'layer3': 0.3, 'layer4': 0.4}\n","\n","        for layer in layer_weights:\n","            if layer in teacher_maps and layer in student_maps:\n","                attn_loss += layer_weights[layer] * self._attention_transfer(\n","                    teacher_maps[layer],\n","                    student_maps[layer]\n","                )\n","\n","        # Distillation loss (KL divergence between probabilities)\n","        teacher_probs = F.softmax(teacher_logits / self.temperature, dim=1)\n","        student_log_probs = F.log_softmax(student_logits / self.temperature, dim=1)\n","\n","        kl_loss = F.kl_div(\n","            student_log_probs,\n","            teacher_probs,\n","            reduction='batchmean'\n","        ) * (self.temperature ** 2)  # Scale by temperature squared\n","\n","        # BCE loss\n","        bce_loss = self.bce_loss(student_logits, targets)\n","\n","        # Combined loss (all components should be positive)\n","        total_loss = (self.alpha * attn_loss +\n","                     self.beta * kl_loss +\n","                     (1 - self.alpha - self.beta) * bce_loss)\n","\n","        return total_loss, {\n","            'attn_loss': attn_loss.item(),\n","            'kl_loss': kl_loss.item(),\n","            'bce_loss': bce_loss.item()\n","        }\n","\n","# Training Function\n","def train_with_attention_transfer(student, teacher, train_loader, val_loader):\n","    optimizer = optim.AdamW(student.parameters(), lr=Config.lr, weight_decay=1e-4)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n","    criterion = AttentionTransferLoss(Config.temperature, Config.alpha, Config.beta)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","\n","    for epoch in range(Config.max_epochs):\n","        student.train()\n","        train_loss = 0\n","        loss_stats = {'attn_loss': 0, 'kl_loss': 0, 'bce_loss': 0}\n","\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","        for step, (images, labels) in enumerate(pbar):\n","            images = images.to(Config.device)\n","            labels = labels.to(Config.device).float()\n","\n","            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n","                # Forward pass\n","                student_logits = student(images)\n","                with torch.no_grad():\n","                    teacher_logits = teacher(images)\n","\n","                # Compute loss\n","                loss, loss_dict = criterion(\n","                    student_logits, teacher_logits,\n","                    student.attention_maps, teacher.attention_maps,\n","                    labels\n","                )\n","                loss = loss / Config.grad_accum_steps\n","\n","            # Backward pass with gradient accumulation\n","            scaler.scale(loss).backward()\n","\n","            if (step + 1) % Config.grad_accum_steps == 0:\n","                # Gradient clipping\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(student.parameters(), Config.grad_clip)\n","\n","                # Update weights\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","\n","            # Update statistics\n","            train_loss += loss.item() * Config.grad_accum_steps\n","            for k in loss_stats:\n","                loss_stats[k] += loss_dict[k]\n","\n","            # Update progress bar\n","            pbar.set_postfix({\n","                'loss': f\"{train_loss/(step+1):.4f}\",\n","                'attn': f\"{loss_stats['attn_loss']/(step+1):.4f}\",\n","                'kl': f\"{loss_stats['kl_loss']/(step+1):.4f}\",\n","                'bce': f\"{loss_stats['bce_loss']/(step+1):.4f}\"\n","            })\n","\n","        # Validation\n","        student.eval()\n","        val_loss = 0\n","        val_stats = {'attn_loss': 0, 'kl_loss': 0, 'bce_loss': 0}\n","\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images = images.to(Config.device)\n","                labels = labels.to(Config.device).float()\n","\n","                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n","                    student_logits = student(images)\n","                    teacher_logits = teacher(images)\n","\n","                    _, loss_dict = criterion(\n","                        student_logits, teacher_logits,\n","                        student.attention_maps, teacher.attention_maps,\n","                        labels\n","                    )\n","\n","                val_loss += loss_dict['attn_loss'] + loss_dict['kl_loss'] + loss_dict['bce_loss']\n","                for k in val_stats:\n","                    val_stats[k] += loss_dict[k]\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","        avg_val_loss = val_loss / len(val_loader)\n","\n","        # Update learning rate\n","        scheduler.step(avg_val_loss)\n","\n","        # Print epoch summary\n","        print(f\"\\nEpoch {epoch+1}/{Config.max_epochs}\")\n","        print(f\"Train Loss: {avg_train_loss:.4f} (Attn: {loss_stats['attn_loss']/len(train_loader):.4f}, \"\n","              f\"KL: {loss_stats['kl_loss']/len(train_loader):.4f}, BCE: {loss_stats['bce_loss']/len(train_loader):.4f})\")\n","        print(f\"Val Loss: {avg_val_loss:.4f} (Attn: {val_stats['attn_loss']/len(val_loader):.4f}, \"\n","              f\"KL: {val_stats['kl_loss']/len(val_loader):.4f}, BCE: {val_stats['bce_loss']/len(val_loader):.4f})\")\n","\n","        # Early stopping check\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","            torch.save({\n","                'model': student.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'epoch': epoch,\n","                'val_loss': best_val_loss\n","            }, 'best_student.pth')\n","            print(\"Saved best model\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= Config.patience:\n","                print(f\"Early stopping at epoch {epoch+1}\")\n","                break\n","\n","    # Load best model\n","    checkpoint = torch.load('best_student.pth')\n","    student.load_state_dict(checkpoint['model'])\n","    return student\n","\n","# Evaluation Function\n","def evaluate_model(model, loader, attribute_names=None):\n","    model.eval()\n","    results = {\n","        'strict_acc': 0.0,\n","        'mean_acc': 0.0,\n","        'top_k_acc': {5: 0.0, 10: 0.0, 20: 0.0},\n","        'per_attribute_acc': np.zeros(Config.num_classes),\n","        'confusion_matrix': np.zeros((2, 2, Config.num_classes))\n","    }\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images = images.to(Config.device)\n","            labels = labels.to(Config.device).float()\n","            batch_size = images.size(0)\n","\n","            outputs = model(images)\n","            probs = torch.sigmoid(outputs)\n","            preds = (probs > 0.5).float()\n","\n","            # Calculate metrics\n","            results['strict_acc'] += (preds == labels).all(dim=1).sum().item()\n","            results['mean_acc'] += (preds == labels).float().mean(dim=1).sum().item()\n","\n","            # Top-k accuracy\n","            for k in results['top_k_acc']:\n","                _, topk = torch.topk(probs, k, dim=1)\n","                correct = torch.gather(labels, 1, topk).sum(dim=1)\n","                results['top_k_acc'][k] += (correct.float() / k).sum().item()\n","\n","            # Per-attribute metrics\n","            for attr in range(Config.num_classes):\n","                attr_pred = preds[:, attr]\n","                attr_label = labels[:, attr]\n","                results['per_attribute_acc'][attr] += (attr_pred == attr_label).sum().item()\n","\n","                # Confusion matrix\n","                tp = ((attr_pred == 1) & (attr_label == 1)).sum().item()\n","                fp = ((attr_pred == 1) & (attr_label == 0)).sum().item()\n","                fn = ((attr_pred == 0) & (attr_label == 1)).sum().item()\n","                tn = ((attr_pred == 0) & (attr_label == 0)).sum().item()\n","                results['confusion_matrix'][0, 0, attr] += tp\n","                results['confusion_matrix'][0, 1, attr] += fp\n","                results['confusion_matrix'][1, 0, attr] += fn\n","                results['confusion_matrix'][1, 1, attr] += tn\n","\n","            total += batch_size\n","\n","    # Normalize metrics\n","    results['strict_acc'] /= total\n","    results['mean_acc'] /= total\n","    for k in results['top_k_acc']:\n","        results['top_k_acc'][k] /= total\n","    results['per_attribute_acc'] /= total\n","\n","    # Calculate per-attribute metrics\n","    attr_metrics = {}\n","    for attr in range(Config.num_classes):\n","        tp = results['confusion_matrix'][0, 0, attr]\n","        fp = results['confusion_matrix'][0, 1, attr]\n","        fn = results['confusion_matrix'][1, 0, attr]\n","        tn = results['confusion_matrix'][1, 1, attr]\n","\n","        precision = tp / (tp + fp + 1e-10)\n","        recall = tp / (tp + fn + 1e-10)\n","        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n","\n","        name = attribute_names[attr] if attribute_names else f\"Attr{attr}\"\n","        attr_metrics[name] = {\n","            'accuracy': (tp + tn) / (tp + tn + fp + fn),\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'support': tp + fn\n","        }\n","\n","    # Print results\n","    print(\"\\n=== Evaluation Results ===\")\n","    print(f\"Strict Accuracy: {results['strict_acc']:.4f}\")\n","    print(f\"Mean Accuracy: {results['mean_acc']:.4f}\")\n","    for k in sorted(results['top_k_acc'].keys()):\n","        print(f\"Top-{k} Accuracy: {results['top_k_acc'][k]:.4f}\")\n","\n","    if attribute_names:\n","        print(\"\\nTop 10 Attributes by F1 Score:\")\n","        print(\"-\" * 85)\n","        print(f\"{'Attribute':<25}{'Accuracy':<10}{'Precision':<10}{'Recall':<10}{'F1':<10}{'Support':<10}\")\n","        print(\"-\" * 85)\n","        sorted_attrs = sorted(attr_metrics.items(), key=lambda x: x[1]['f1'], reverse=True)[:10]\n","        for name, metrics in sorted_attrs:\n","            print(f\"{name:<25}{metrics['accuracy']:.4f}    {metrics['precision']:.4f}    \"\n","                  f\"{metrics['recall']:.4f}    {metrics['f1']:.4f}    {metrics['support']:>10}\")\n","\n","    return results, attr_metrics\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Initialize data loaders (replace with your actual data loaders)\n","    # train_loader = ...\n","    # val_loader = ...\n","    # test_loader = ...\n","\n","    # Initialize models\n","    print(\"Initializing models...\")\n","    teacher = TeacherModel(Config.num_classes).to(Config.device)\n","    student = StudentModel(Config.num_classes).to(Config.device)\n","\n","    # Load teacher weights with proper handling\n","    print(\"Loading teacher weights...\")\n","    teacher_weights = torch.load('/content/drive/MyDrive/best_model.pth', map_location=Config.device)\n","\n","    # Handle potential DataParallel and key mismatches\n","    if all(k.startswith('module.') for k in teacher_weights.keys()):\n","        teacher_weights = OrderedDict([(k.replace('module.', ''), v) for k, v in teacher_weights.items()])\n","\n","    # Load weights into model\n","    teacher.load_state_dict(teacher_weights, strict=False)\n","    teacher.eval()\n","\n","    # Train with attention transfer\n","    print(\"\\nStarting attention transfer training...\")\n","    trained_student = train_with_attention_transfer(student, teacher, train_loader, valid_loader)\n","\n","    # Evaluate\n","    print(\"\\nEvaluating on test set...\")\n","    test_results, attr_metrics = evaluate_model(trained_student, test_loader, Config.attribute_names)\n","\n","    # Save final model\n","    torch.save(trained_student.state_dict(), 'final_student.pth')\n","    print(\"\\nTraining complete. Saved final student model.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yfm2jL1ALzWk","outputId":"4899295e-ad3d-45f8-ffc3-4e6ba90290eb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing models...\n","Loading teacher weights...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-2e24fd5de71a>:165: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["\n","Starting attention transfer training...\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 2544/2544 [05:25<00:00,  7.81it/s, loss=0.4468, attn=0.0007, kl=0.2653, bce=0.9174]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/30\n","Train Loss: 0.4468 (Attn: 0.0007, KL: 0.2653, BCE: 0.9174)\n","Val Loss: 1.0881 (Attn: 0.0004, KL: 0.2206, BCE: 0.8671)\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 2544/2544 [05:22<00:00,  7.88it/s, loss=0.4229, attn=0.0004, kl=0.2194, bce=0.8924]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/30\n","Train Loss: 0.4229 (Attn: 0.0004, KL: 0.2194, BCE: 0.8924)\n","Val Loss: 1.0723 (Attn: 0.0004, KL: 0.1957, BCE: 0.8762)\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 2544/2544 [05:23<00:00,  7.86it/s, loss=0.4196, attn=0.0003, kl=0.2187, bce=0.8847]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/30\n","Train Loss: 0.4196 (Attn: 0.0003, KL: 0.2187, BCE: 0.8847)\n","Val Loss: 1.0809 (Attn: 0.0003, KL: 0.2264, BCE: 0.8542)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 2544/2544 [05:22<00:00,  7.88it/s, loss=0.4175, attn=0.0003, kl=0.2199, bce=0.8787]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/30\n","Train Loss: 0.4175 (Attn: 0.0003, KL: 0.2199, BCE: 0.8787)\n","Val Loss: 1.0754 (Attn: 0.0003, KL: 0.2134, BCE: 0.8616)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|██████████| 2544/2544 [05:22<00:00,  7.88it/s, loss=0.4153, attn=0.0003, kl=0.2207, bce=0.8726]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/30\n","Train Loss: 0.4153 (Attn: 0.0003, KL: 0.2207, BCE: 0.8726)\n","Val Loss: 1.0923 (Attn: 0.0003, KL: 0.2518, BCE: 0.8402)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 2544/2544 [05:21<00:00,  7.91it/s, loss=0.4130, attn=0.0004, kl=0.2222, bce=0.8655]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/30\n","Train Loss: 0.4130 (Attn: 0.0004, KL: 0.2222, BCE: 0.8655)\n","Val Loss: 1.0797 (Attn: 0.0004, KL: 0.2243, BCE: 0.8550)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|██████████| 2544/2544 [05:21<00:00,  7.90it/s, loss=0.4073, attn=0.0004, kl=0.2230, bce=0.8506]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/30\n","Train Loss: 0.4073 (Attn: 0.0004, KL: 0.2230, BCE: 0.8506)\n","Val Loss: 1.0815 (Attn: 0.0004, KL: 0.2184, BCE: 0.8628)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|██████████| 2544/2544 [05:22<00:00,  7.89it/s, loss=0.4043, attn=0.0004, kl=0.2259, bce=0.8410]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/30\n","Train Loss: 0.4043 (Attn: 0.0004, KL: 0.2259, BCE: 0.8410)\n","Val Loss: 1.0865 (Attn: 0.0004, KL: 0.2284, BCE: 0.8578)\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|██████████| 2544/2544 [05:22<00:00,  7.88it/s, loss=0.4016, attn=0.0004, kl=0.2276, bce=0.8331]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/30\n","Train Loss: 0.4016 (Attn: 0.0004, KL: 0.2276, BCE: 0.8331)\n","Val Loss: 1.0905 (Attn: 0.0004, KL: 0.2340, BCE: 0.8561)\n","Early stopping at epoch 9\n","\n","Evaluating on test set...\n","\n","=== Evaluation Results ===\n","Strict Accuracy: 0.0000\n","Mean Accuracy: 0.6929\n","Top-5 Accuracy: 0.2111\n","Top-10 Accuracy: 0.2963\n","Top-20 Accuracy: 0.3463\n","\n","Top 10 Attributes by F1 Score:\n","-------------------------------------------------------------------------------------\n","Attribute                Accuracy  Precision Recall    F1        Support   \n","-------------------------------------------------------------------------------------\n","Mouth_Slightly_Open      0.7373    0.6728    0.9137    0.7750        9883.0\n","High_Cheekbones          0.4818    0.4818    1.0000    0.6503        9618.0\n","Heavy_Makeup             0.4995    0.4471    0.9964    0.6172        8084.0\n","Smiling                  0.6402    0.9996    0.2810    0.4386        9987.0\n","Oval_Face                0.7261    0.5613    0.3367    0.4209        5901.0\n","Bangs                    0.6472    0.2770    0.7858    0.4096        3109.0\n","Blond_Hair               0.8951    0.9639    0.2207    0.3591        2660.0\n","Attractive               0.5497    0.9710    0.0947    0.1725        9898.0\n","Bushy_Eyebrows           0.8791    0.9479    0.0704    0.1310        2586.0\n","Goatee                   0.4893    0.0696    0.8197    0.1283         915.0\n","\n","Training complete. Saved final student model.\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import models\n","from tqdm import tqdm\n","import numpy as np\n","from collections import OrderedDict\n","\n","# Configuration\n","class Config:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    num_classes = 40\n","    batch_size = 64\n","    lr = 5e-5  # Reduced learning rate\n","    max_epochs = 50\n","    temperature = 1.0  # More conservative temperature\n","    alpha = 0.2  # Reduced attention transfer weight\n","    beta = 0.2   # Reduced distillation weight\n","    patience = 10\n","    grad_clip = 0.1  # Tighter gradient clipping\n","    grad_accum_steps = 4\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","\n","    # Class weights - should be calculated from your dataset statistics\n","    pos_weight = torch.tensor([\n","        0.5, 0.7, 0.6, 0.8, 0.3, 0.4, 0.6, 0.7, 0.5, 0.4,\n","        0.3, 0.5, 0.4, 0.6, 0.3, 0.2, 0.3, 0.4, 0.5, 0.6,\n","        0.7, 0.5, 0.3, 0.4, 0.8, 0.5, 0.3, 0.5, 0.4, 0.3,\n","        0.3, 0.6, 0.5, 0.5, 0.4, 0.3, 0.6, 0.4, 0.3, 0.7\n","    ]).to(device)\n","\n","    attribute_names = [\n","        '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n","        'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n","        'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n","        'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n","        'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n","        'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks',\n","        'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n","        'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n","    ]\n","\n","# Teacher Model\n","class TeacherModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.base_model = models.resnet50(weights=None)\n","\n","        self.base_model.fc = nn.Sequential(\n","            nn.Dropout(0.4),\n","            nn.Linear(self.base_model.fc.in_features, 2048),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(2048),\n","            nn.Dropout(0.3),\n","            nn.Linear(2048, 1024),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(1024),\n","            nn.Linear(1024, num_classes)\n","        )\n","\n","        self.attention_maps = {}\n","        self._register_hooks()\n","\n","    def _register_hooks(self):\n","        def get_activation(name):\n","            def hook(model, input, output):\n","                self.attention_maps[name] = output.detach()\n","            return hook\n","\n","        self.base_model.layer1.register_forward_hook(get_activation('layer1'))\n","        self.base_model.layer2.register_forward_hook(get_activation('layer2'))\n","        self.base_model.layer3.register_forward_hook(get_activation('layer3'))\n","        self.base_model.layer4.register_forward_hook(get_activation('layer4'))\n","\n","    def forward(self, x):\n","        self.attention_maps.clear()\n","        return self.base_model(x)\n","\n","# Student Model\n","class StudentModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.base_model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","\n","        self.base_model.fc = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(self.base_model.fc.in_features, 1024),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(1024),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(512),\n","            nn.Linear(512, num_classes)\n","        )\n","\n","        self.attention_maps = {}\n","        self._register_hooks()\n","\n","    def _register_hooks(self):\n","        def get_activation(name):\n","            def hook(model, input, output):\n","                self.attention_maps[name] = output\n","            return hook\n","\n","        self.base_model.layer1.register_forward_hook(get_activation('layer1'))\n","        self.base_model.layer2.register_forward_hook(get_activation('layer2'))\n","        self.base_model.layer3.register_forward_hook(get_activation('layer3'))\n","        self.base_model.layer4.register_forward_hook(get_activation('layer4'))\n","\n","    def forward(self, x):\n","        self.attention_maps.clear()\n","        return self.base_model(x)\n","\n","# Corrected Attention Transfer Loss\n","class AttentionTransferLoss(nn.Module):\n","    def __init__(self, temperature=1.0, alpha=0.2, beta=0.2):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.bce_loss = nn.BCEWithLogitsLoss(pos_weight=Config.pos_weight)\n","\n","    def _attention_transfer(self, teacher_feat, student_feat):\n","        \"\"\"Handle different feature map sizes safely\"\"\"\n","        # Resize teacher features to match student dimensions\n","        if teacher_feat.shape[2:] != student_feat.shape[2:]:\n","            teacher_feat = F.adaptive_avg_pool2d(teacher_feat, student_feat.shape[2:])\n","\n","        # Compute normalized attention maps\n","        teacher_attention = F.normalize(teacher_feat.pow(2).mean(1).view(teacher_feat.size(0), -1), p=2, dim=1)\n","        student_attention = F.normalize(student_feat.pow(2).mean(1).view(student_feat.size(0), -1), p=2, dim=1)\n","\n","        return F.mse_loss(student_attention, teacher_attention)\n","\n","    def forward(self, student_logits, teacher_logits, student_maps, teacher_maps, targets):\n","        # Attention transfer loss\n","        attn_loss = 0\n","        layer_pairs = [\n","            ('layer1', 'layer1', 0.1),\n","            ('layer2', 'layer2', 0.2),\n","            ('layer3', 'layer3', 0.3),\n","            ('layer4', 'layer4', 0.4)\n","        ]\n","\n","        for t_layer, s_layer, weight in layer_pairs:\n","            if t_layer in teacher_maps and s_layer in student_maps:\n","                attn_loss += weight * self._attention_transfer(\n","                    teacher_maps[t_layer],\n","                    student_maps[s_layer]\n","                )\n","\n","        # Stable KL Divergence calculation\n","        teacher_probs = torch.sigmoid(teacher_logits)\n","        student_probs = torch.sigmoid(student_logits)\n","\n","        # Temperature scaling with numerical stability\n","        teacher_probs = teacher_probs.pow(1/self.temperature)\n","        teacher_probs = teacher_probs / (teacher_probs.sum(dim=1, keepdim=True) + 1e-10)\n","\n","        student_probs = student_probs.pow(1/self.temperature)\n","        student_probs = student_probs / (student_probs.sum(dim=1, keepdim=True) + 1e-10)\n","\n","        kl_loss = F.kl_div(\n","            torch.log(student_probs + 1e-10),\n","            teacher_probs,\n","            reduction='batchmean'\n","        )\n","\n","        # BCE loss\n","        bce_loss = self.bce_loss(student_logits, targets)\n","\n","        # Combined loss with safeguards\n","        total_loss = (self.alpha * attn_loss +\n","                     self.beta * kl_loss +\n","                     (1 - self.alpha - self.beta) * bce_loss)\n","\n","        # Verify all losses are finite\n","        if not torch.isfinite(total_loss):\n","            print(f\"Warning: Non-finite loss detected - attn: {attn_loss}, kl: {kl_loss}, bce: {bce_loss}\")\n","            total_loss = bce_loss  # Fall back to BCE only if other losses become unstable\n","\n","        return total_loss, {\n","            'attn_loss': attn_loss.item() if torch.isfinite(attn_loss) else 0,\n","            'kl_loss': kl_loss.item() if torch.isfinite(kl_loss) else 0,\n","            'bce_loss': bce_loss.item()\n","        }\n","\n","# Training Function\n","def train_with_attention_transfer(student, teacher, train_loader, val_loader):\n","    optimizer = optim.AdamW(student.parameters(), lr=Config.lr, weight_decay=1e-4)\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode='min',\n","        factor=0.5,\n","        patience=3,\n","        verbose=True\n","    )\n","    criterion = AttentionTransferLoss(Config.temperature, Config.alpha, Config.beta)\n","    scaler = torch.amp.GradScaler()\n","\n","    best_val_loss = float('inf')\n","    best_val_acc = 0.0\n","    patience_counter = 0\n","\n","    for epoch in range(Config.max_epochs):\n","        student.train()\n","        train_loss = 0\n","        loss_stats = {'attn_loss': 0, 'kl_loss': 0, 'bce_loss': 0}\n","\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","        for step, (images, labels) in enumerate(pbar):\n","            images = images.to(Config.device)\n","            labels = labels.to(Config.device).float()\n","\n","            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n","                student_logits = student(images)\n","                with torch.no_grad():\n","                    teacher_logits = teacher(images)\n","\n","                loss, loss_dict = criterion(\n","                    student_logits, teacher_logits,\n","                    student.attention_maps, teacher.attention_maps,\n","                    labels\n","                )\n","\n","                if not torch.isfinite(loss):\n","                    print(f\"Non-finite loss detected at step {step}\")\n","                    optimizer.zero_grad()\n","                    break\n","\n","                loss = loss / Config.grad_accum_steps\n","\n","            scaler.scale(loss).backward()\n","\n","            if (step + 1) % Config.grad_accum_steps == 0:\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(student.parameters(), Config.grad_clip)\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","\n","            train_loss += loss.item() * Config.grad_accum_steps\n","            for k in loss_stats:\n","                loss_stats[k] += loss_dict[k]\n","\n","            pbar.set_postfix({\n","                'loss': f\"{train_loss/(step+1):.4f}\",\n","                'attn': f\"{loss_stats['attn_loss']/(step+1):.4f}\",\n","                'kl': f\"{loss_stats['kl_loss']/(step+1):.4f}\",\n","                'bce': f\"{loss_stats['bce_loss']/(step+1):.4f}\"\n","            })\n","\n","        # Validation\n","        student.eval()\n","        val_loss = 0\n","        val_stats = {'attn_loss': 0, 'kl_loss': 0, 'bce_loss': 0}\n","        val_mean_acc = 0\n","\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images = images.to(Config.device)\n","                labels = labels.to(Config.device).float()\n","\n","                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n","                    student_logits = student(images)\n","                    teacher_logits = teacher(images)\n","\n","                    _, loss_dict = criterion(\n","                        student_logits, teacher_logits,\n","                        student.attention_maps, teacher.attention_maps,\n","                        labels\n","                    )\n","\n","                preds = (torch.sigmoid(student_logits) > 0.5).float()\n","                val_mean_acc += (preds == labels).float().mean(dim=1).sum().item()\n","\n","                val_loss += loss_dict['attn_loss'] + loss_dict['kl_loss'] + loss_dict['bce_loss']\n","                for k in val_stats:\n","                    val_stats[k] += loss_dict[k]\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","        avg_val_loss = val_loss / len(val_loader)\n","        val_mean_acc /= len(val_loader.dataset)\n","\n","        scheduler.step(avg_val_loss)\n","\n","        print(f\"\\nEpoch {epoch+1}/{Config.max_epochs}\")\n","        print(f\"Train Loss: {avg_train_loss:.4f} (Attn: {loss_stats['attn_loss']/len(train_loader):.4f}, \"\n","              f\"KL: {loss_stats['kl_loss']/len(train_loader):.4f}, BCE: {loss_stats['bce_loss']/len(train_loader):.4f})\")\n","        print(f\"Val Loss: {avg_val_loss:.4f} (Attn: {val_stats['attn_loss']/len(val_loader):.4f}, \"\n","              f\"KL: {val_stats['kl_loss']/len(val_loader):.4f}, BCE: {val_stats['bce_loss']/len(val_loader):.4f})\")\n","        print(f\"Val Mean Accuracy: {val_mean_acc:.4f}\")\n","\n","        # Early stopping\n","        if avg_val_loss < best_val_loss or val_mean_acc > best_val_acc:\n","            if avg_val_loss < best_val_loss:\n","                best_val_loss = avg_val_loss\n","            if val_mean_acc > best_val_acc:\n","                best_val_acc = val_mean_acc\n","            patience_counter = 0\n","            torch.save({\n","                'model': student.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'epoch': epoch,\n","                'val_loss': best_val_loss,\n","                'val_acc': best_val_acc\n","            }, 'best_student.pth')\n","            print(\"Saved best model\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= Config.patience:\n","                print(f\"Early stopping at epoch {epoch+1}\")\n","                break\n","\n","    checkpoint = torch.load('best_student.pth')\n","    student.load_state_dict(checkpoint['model'])\n","    return student\n","\n","# Evaluation Function\n","def evaluate_model(model, loader, attribute_names=None):\n","    model.eval()\n","    results = {\n","        'strict_acc': 0.0,\n","        'mean_acc': 0.0,\n","        'top_k_acc': {3: 0.0, 5: 0.0, 10: 0.0},\n","        'per_attribute_acc': np.zeros(Config.num_classes),\n","        'per_attribute_f1': np.zeros(Config.num_classes),\n","        'confusion_matrix': np.zeros((2, 2, Config.num_classes))\n","    }\n","    total = 0\n","\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images = images.to(Config.device)\n","            labels = labels.to(Config.device).float()\n","            batch_size = images.size(0)\n","\n","            outputs = model(images)\n","            probs = torch.sigmoid(outputs)\n","            preds = (probs > 0.5).float()\n","\n","            results['strict_acc'] += (preds == labels).all(dim=1).sum().item()\n","            results['mean_acc'] += (preds == labels).float().mean(dim=1).sum().item()\n","\n","            for k in results['top_k_acc']:\n","                _, topk = torch.topk(probs, k, dim=1)\n","                correct = torch.gather(labels, 1, topk).sum(dim=1)\n","                results['top_k_acc'][k] += (correct.float() / k).sum().item()\n","\n","            for attr in range(Config.num_classes):\n","                attr_pred = preds[:, attr]\n","                attr_label = labels[:, attr]\n","                results['per_attribute_acc'][attr] += (attr_pred == attr_label).sum().item()\n","\n","                tp = ((attr_pred == 1) & (attr_label == 1)).sum().item()\n","                fp = ((attr_pred == 1) & (attr_label == 0)).sum().item()\n","                fn = ((attr_pred == 0) & (attr_label == 1)).sum().item()\n","                tn = ((attr_pred == 0) & (attr_label == 0)).sum().item()\n","                results['confusion_matrix'][0, 0, attr] += tp\n","                results['confusion_matrix'][0, 1, attr] += fp\n","                results['confusion_matrix'][1, 0, attr] += fn\n","                results['confusion_matrix'][1, 1, attr] += tn\n","\n","                precision = tp / (tp + fp + 1e-10)\n","                recall = tp / (tp + fn + 1e-10)\n","                f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n","                results['per_attribute_f1'][attr] += f1 * batch_size\n","\n","            total += batch_size\n","\n","    results['strict_acc'] /= total\n","    results['mean_acc'] /= total\n","    for k in results['top_k_acc']:\n","        results['top_k_acc'][k] /= total\n","    results['per_attribute_acc'] /= total\n","    results['per_attribute_f1'] /= total\n","\n","    attr_metrics = {}\n","    for attr in range(Config.num_classes):\n","        tp = results['confusion_matrix'][0, 0, attr]\n","        fp = results['confusion_matrix'][0, 1, attr]\n","        fn = results['confusion_matrix'][1, 0, attr]\n","        tn = results['confusion_matrix'][1, 1, attr]\n","\n","        precision = tp / (tp + fp + 1e-10)\n","        recall = tp / (tp + fn + 1e-10)\n","        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n","\n","        name = attribute_names[attr] if attribute_names else f\"Attr{attr}\"\n","        attr_metrics[name] = {\n","            'accuracy': (tp + tn) / (tp + tn + fp + fn),\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'support': tp + fn\n","        }\n","\n","    print(\"\\n=== Evaluation Results ===\")\n","    print(f\"Strict Accuracy: {results['strict_acc']:.4f}\")\n","    print(f\"Mean Accuracy: {results['mean_acc']:.4f}\")\n","    for k in sorted(results['top_k_acc'].keys()):\n","        print(f\"Top-{k} Accuracy: {results['top_k_acc'][k]:.4f}\")\n","\n","    if attribute_names:\n","        print(\"\\nTop 10 Attributes by F1 Score:\")\n","        print(\"-\" * 85)\n","        print(f\"{'Attribute':<25}{'Accuracy':<10}{'Precision':<10}{'Recall':<10}{'F1':<10}{'Support':<10}\")\n","        print(\"-\" * 85)\n","        sorted_attrs = sorted(attr_metrics.items(), key=lambda x: x[1]['f1'], reverse=True)[:10]\n","        for name, metrics in sorted_attrs:\n","            print(f\"{name:<25}{metrics['accuracy']:.4f}    {metrics['precision']:.4f}    \"\n","                  f\"{metrics['recall']:.4f}    {metrics['f1']:.4f}    {metrics['support']:>10}\")\n","\n","    return results, attr_metrics\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Initialize data loaders (replace with your actual data loaders)\n","    # train_loader = ...\n","    # val_loader = ...\n","    # test_loader = ...\n","\n","    # Initialize models\n","    print(\"Initializing models...\")\n","    teacher = TeacherModel(Config.num_classes).to(Config.device)\n","    student = StudentModel(Config.num_classes).to(Config.device)\n","\n","    # Load teacher weights\n","    print(\"Loading teacher weights...\")\n","    teacher_weights = torch.load('/content/drive/MyDrive/best_model.pth', map_location=Config.device)\n","\n","    if all(k.startswith('module.') for k in teacher_weights.keys()):\n","        teacher_weights = OrderedDict([(k.replace('module.', ''), v) for k, v in teacher_weights.items()])\n","\n","    teacher.load_state_dict(teacher_weights, strict=False)\n","    teacher.eval()\n","\n","    # Train with attention transfer\n","    print(\"\\nStarting attention transfer training...\")\n","    trained_student = train_with_attention_transfer(student, teacher, train_loader, valid_loader)\n","\n","    # Evaluate\n","    print(\"\\nEvaluating on test set...\")\n","    test_results, attr_metrics = evaluate_model(trained_student, test_loader, Config.attribute_names)\n","\n","    # Save final model\n","    torch.save({\n","        'model': trained_student.state_dict(),\n","        'config': Config.__dict__,\n","        'test_results': test_results\n","    }, 'final_student.pth')\n","    print(\"\\nTraining complete. Saved final student model.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"EYgWAjAsqITF","outputId":"e1701e17-5d15-4473-c608-d1708cb0f379"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing models...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loading teacher weights...\n","\n","Starting attention transfer training...\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 2544/2544 [05:28<00:00,  7.74it/s, loss=0.3285, attn=0.0032, kl=0.1486, bce=0.4969]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/50\n","Train Loss: 0.3285 (Attn: 0.0032, KL: 0.1486, BCE: 0.4969)\n","Val Loss: 0.5367 (Attn: 0.0025, KL: 0.1515, BCE: 0.3827)\n","Val Mean Accuracy: 0.8921\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 2544/2544 [05:28<00:00,  7.74it/s, loss=0.2233, attn=0.0021, kl=0.1821, bce=0.3107]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/50\n","Train Loss: 0.2233 (Attn: 0.0021, KL: 0.1821, BCE: 0.3107)\n","Val Loss: 0.4593 (Attn: 0.0018, KL: 0.2098, BCE: 0.2477)\n","Val Mean Accuracy: 0.8886\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 2544/2544 [05:29<00:00,  7.73it/s, loss=0.1903, attn=0.0016, kl=0.2180, bce=0.2440]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/50\n","Train Loss: 0.1903 (Attn: 0.0016, KL: 0.2180, BCE: 0.2440)\n","Val Loss: 0.4553 (Attn: 0.0015, KL: 0.2228, BCE: 0.2311)\n","Val Mean Accuracy: 0.8764\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 2544/2544 [05:32<00:00,  7.64it/s, loss=0.1854, attn=0.0013, kl=0.2108, bce=0.2382]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/50\n","Train Loss: 0.1854 (Attn: 0.0013, KL: 0.2108, BCE: 0.2382)\n","Val Loss: 0.4368 (Attn: 0.0012, KL: 0.2014, BCE: 0.2342)\n","Val Mean Accuracy: 0.8654\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|██████████| 2544/2544 [05:33<00:00,  7.62it/s, loss=0.1835, attn=0.0012, kl=0.2042, bce=0.2373]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/50\n","Train Loss: 0.1835 (Attn: 0.0012, KL: 0.2042, BCE: 0.2373)\n","Val Loss: 0.4337 (Attn: 0.0011, KL: 0.1988, BCE: 0.2338)\n","Val Mean Accuracy: 0.8625\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 2544/2544 [05:33<00:00,  7.63it/s, loss=0.1822, attn=0.0011, kl=0.2007, bce=0.2364]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/50\n","Train Loss: 0.1822 (Attn: 0.0011, KL: 0.2007, BCE: 0.2364)\n","Val Loss: 0.4368 (Attn: 0.0010, KL: 0.2044, BCE: 0.2313)\n","Val Mean Accuracy: 0.8651\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|██████████| 2544/2544 [05:34<00:00,  7.61it/s, loss=0.1812, attn=0.0010, kl=0.1998, bce=0.2350]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/50\n","Train Loss: 0.1812 (Attn: 0.0010, KL: 0.1998, BCE: 0.2350)\n","Val Loss: 0.4342 (Attn: 0.0010, KL: 0.2008, BCE: 0.2324)\n","Val Mean Accuracy: 0.8643\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|██████████| 2544/2544 [05:33<00:00,  7.63it/s, loss=0.1803, attn=0.0010, kl=0.1995, bce=0.2336]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/50\n","Train Loss: 0.1803 (Attn: 0.0010, KL: 0.1995, BCE: 0.2336)\n","Val Loss: 0.4289 (Attn: 0.0010, KL: 0.1939, BCE: 0.2340)\n","Val Mean Accuracy: 0.8600\n","Saved best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|██████████| 2544/2544 [05:29<00:00,  7.72it/s, loss=0.1793, attn=0.0010, kl=0.2004, bce=0.2318]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/50\n","Train Loss: 0.1793 (Attn: 0.0010, KL: 0.2004, BCE: 0.2318)\n","Val Loss: 0.4419 (Attn: 0.0010, KL: 0.2042, BCE: 0.2368)\n","Val Mean Accuracy: 0.8627\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|██████████| 2544/2544 [05:29<00:00,  7.72it/s, loss=0.1782, attn=0.0009, kl=0.2007, bce=0.2297]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10/50\n","Train Loss: 0.1782 (Attn: 0.0009, KL: 0.2007, BCE: 0.2297)\n","Val Loss: 0.4313 (Attn: 0.0009, KL: 0.1972, BCE: 0.2332)\n","Val Mean Accuracy: 0.8613\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11: 100%|██████████| 2544/2544 [05:28<00:00,  7.75it/s, loss=0.1770, attn=0.0009, kl=0.2016, bce=0.2275]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11/50\n","Train Loss: 0.1770 (Attn: 0.0009, KL: 0.2016, BCE: 0.2275)\n","Val Loss: 0.4408 (Attn: 0.0010, KL: 0.2068, BCE: 0.2331)\n","Val Mean Accuracy: 0.8681\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12: 100%|██████████| 2544/2544 [05:27<00:00,  7.76it/s, loss=0.1760, attn=0.0010, kl=0.2036, bce=0.2252]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 12/50\n","Train Loss: 0.1760 (Attn: 0.0010, KL: 0.2036, BCE: 0.2252)\n","Val Loss: 0.4384 (Attn: 0.0009, KL: 0.2056, BCE: 0.2318)\n","Val Mean Accuracy: 0.8657\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13: 100%|██████████| 2544/2544 [05:29<00:00,  7.71it/s, loss=0.1744, attn=0.0010, kl=0.2065, bce=0.2214]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 13/50\n","Train Loss: 0.1744 (Attn: 0.0010, KL: 0.2065, BCE: 0.2214)\n","Val Loss: 0.4394 (Attn: 0.0010, KL: 0.2064, BCE: 0.2320)\n","Val Mean Accuracy: 0.8664\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14: 100%|██████████| 2544/2544 [05:29<00:00,  7.73it/s, loss=0.1736, attn=0.0010, kl=0.2090, bce=0.2194]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 14/50\n","Train Loss: 0.1736 (Attn: 0.0010, KL: 0.2090, BCE: 0.2194)\n","Val Loss: 0.4400 (Attn: 0.0010, KL: 0.2062, BCE: 0.2328)\n","Val Mean Accuracy: 0.8660\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15: 100%|██████████| 2544/2544 [05:28<00:00,  7.75it/s, loss=0.1730, attn=0.0010, kl=0.2110, bce=0.2177]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 15/50\n","Train Loss: 0.1730 (Attn: 0.0010, KL: 0.2110, BCE: 0.2177)\n","Val Loss: 0.4422 (Attn: 0.0010, KL: 0.2084, BCE: 0.2328)\n","Val Mean Accuracy: 0.8657\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16: 100%|██████████| 2544/2544 [05:27<00:00,  7.77it/s, loss=0.1724, attn=0.0010, kl=0.2130, bce=0.2160]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 16/50\n","Train Loss: 0.1724 (Attn: 0.0010, KL: 0.2130, BCE: 0.2160)\n","Val Loss: 0.4469 (Attn: 0.0010, KL: 0.2137, BCE: 0.2321)\n","Val Mean Accuracy: 0.8689\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17:   5%|▍         | 120/2544 [00:15<05:21,  7.53it/s, loss=0.1718, attn=0.0010, kl=0.2140, bce=0.2146]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-117fc0aaffc0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;31m# Train with attention transfer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStarting attention transfer training...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mtrained_student\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_with_attention_transfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-117fc0aaffc0>\u001b[0m in \u001b[0;36mtrain_with_attention_transfer\u001b[0;34m(student, teacher, train_loader, val_loader)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-1-aee666df0010>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mattrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got inappropriate size arg: {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 )\n\u001b[1;32m   2355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m     def reduce(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["# Load the model architecture first\n","student = StudentModel(Config.num_classes).to(Config.device)\n","\n","# Load weights from checkpoint\n","checkpoint = torch.load('best_student.pth')  # or 'final_student.pth'\n","student.load_state_dict(checkpoint['model'])\n","student.eval()  # Set to evaluation mode\n","test_results, attr_metrics = evaluate_model(student, test_loader, Config.attribute_names)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5kIME_XMADI","outputId":"7ed0ad54-6ab8-4192-9521-92a3e10b1f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Evaluation Results ===\n","Strict Accuracy: 0.0024\n","Mean Accuracy: 0.8533\n","Top-3 Accuracy: 0.9419\n","Top-5 Accuracy: 0.8767\n","Top-10 Accuracy: 0.6915\n","\n","Top 10 Attributes by F1 Score:\n","-------------------------------------------------------------------------------------\n","Attribute                Accuracy  Precision Recall    F1        Support   \n","-------------------------------------------------------------------------------------\n","Wearing_Lipstick         0.8577    0.9866    0.7373    0.8439       10418.0\n","Mouth_Slightly_Open      0.8532    0.9896    0.7109    0.8274        9883.0\n","High_Cheekbones          0.8265    0.9699    0.6603    0.7857        9618.0\n","No_Beard                 0.6959    0.9932    0.6482    0.7844       17041.0\n","Heavy_Makeup             0.8425    0.9750    0.6270    0.7632        8084.0\n","Attractive               0.7649    0.9043    0.5880    0.7126        9898.0\n","Smiling                  0.7647    0.9981    0.5306    0.6929        9987.0\n","Male                     0.7989    0.9962    0.4815    0.6492        7715.0\n","Bangs                    0.9027    0.9787    0.3837    0.5513        3109.0\n","Wavy_Hair                0.7680    0.9376    0.3886    0.5495        7267.0\n"]}]}]}