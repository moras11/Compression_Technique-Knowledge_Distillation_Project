{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import zipfile\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torchvision import transforms\n","from PIL import Image\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","# 1. Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# 2. Extract CelebA ZIP (update your path)\n","zip_path = '/content/drive/MyDrive/img_align_celeba.zip'  # Change to your path\n","extract_path = '/content/celeba'\n","\n","if not os.path.exists(extract_path):\n","    os.makedirs(extract_path, exist_ok=True)\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        for file in tqdm(zip_ref.namelist(), desc='Extracting'):\n","            zip_ref.extract(file, extract_path)\n","\n","# 3. Dataset Class with Splitting Capability\n","class CelebADataset(Dataset):\n","    def __init__(self, root_dir, split='train', transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","                                      delim_whitespace=True, header=1)\n","        self.filenames = self.attributes.index.tolist()\n","        self.attributes = (self.attributes + 1) // 2  # Convert -1/1 to 0/1\n","\n","        # Load the official split file\n","        split_file = os.path.join(root_dir, 'list_eval_partition.txt')\n","        split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n","\n","        # Filter based on requested split\n","        if split == 'train':\n","            self.filenames = [f for f in self.filenames if split_info.loc[f].partition == 0]\n","        elif split == 'valid':\n","            self.filenames = [f for f in self.filenames if split_info.loc[f].partition == 1]\n","        elif split == 'test':\n","            self.filenames = [f for f in self.filenames if split_info.loc[f].partition == 2]\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.root_dir, 'img_align_celeba', self.filenames[idx])\n","        try:\n","            img = Image.open(img_path).convert('RGB')\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            img = torch.zeros(3, 128, 128)  # Return a blank image on failure\n","\n","        attrs = self.attributes.loc[self.filenames[idx]].values.astype('float32')\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, torch.from_numpy(attrs)\n","\n","# 4. Define Transforms\n","transform = transforms.Compose([\n","    transforms.Resize(128),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","# 5. Create DataLoaders\n","train_dataset = CelebADataset(extract_path, split='train', transform=transform)\n","valid_dataset = CelebADataset(extract_path, split='valid', transform=transform)\n","test_dataset = CelebADataset(extract_path, split='test', transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Verify\n","images, attrs = next(iter(train_loader))\n","print(f\"Train batch: {images.shape}, {attrs.shape}\")\n","print(f\"Train samples: {len(train_loader.dataset)}, Valid samples: {len(valid_loader.dataset)}, Test samples: {len(test_loader.dataset)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JpmQW0liF5YE","outputId":"a9eead97-81dd-4863-e6ed-9e1600681b48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-f02caa93b373>:29: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","<ipython-input-2-f02caa93b373>:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n","<ipython-input-2-f02caa93b373>:29: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","<ipython-input-2-f02caa93b373>:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n","<ipython-input-2-f02caa93b373>:29: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","<ipython-input-2-f02caa93b373>:36: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  split_info = pd.read_csv(split_file, delim_whitespace=True, header=None, index_col=0, names=['partition'])\n"]},{"output_type":"stream","name":"stdout","text":["Train batch: torch.Size([64, 3, 156, 128]), torch.Size([64, 40])\n","Train samples: 162770, Valid samples: 19867, Test samples: 19962\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y08EzRyKFC5m","outputId":"4ed0b087-1c16-41f8-f95b-6dea53779ea6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading teacher model...\n","Initializing student model...\n"]},{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 118MB/s]\n","/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\n","<ipython-input-8-930bf041bc5e>:116: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["\n","Starting knowledge distillation training...\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch 1:   0%|          | 0/2544 [00:00<?, ?it/s]<ipython-input-8-930bf041bc5e>:132: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n","Epoch 1: 100%|██████████| 2544/2544 [08:21<00:00,  5.07it/s, loss=0.2638, kl=0.0572, bce=0.4704]\n","<ipython-input-8-930bf041bc5e>:166: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast():\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1/30\n","Train Loss: 0.2638 (KL: 0.0572, BCE: 0.4704)\n","Val Loss: 0.2580 (KL: 0.0593, BCE: 0.4567)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 2544/2544 [07:32<00:00,  5.62it/s, loss=0.2557, kl=0.0589, bce=0.4524]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 2/30\n","Train Loss: 0.2557 (KL: 0.0589, BCE: 0.4524)\n","Val Loss: 0.2558 (KL: 0.0569, BCE: 0.4548)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 2544/2544 [07:27<00:00,  5.68it/s, loss=0.2526, kl=0.0595, bce=0.4456]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 3/30\n","Train Loss: 0.2526 (KL: 0.0595, BCE: 0.4456)\n","Val Loss: 0.2558 (KL: 0.0619, BCE: 0.4496)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 2544/2544 [07:31<00:00,  5.63it/s, loss=0.2497, kl=0.0603, bce=0.4392]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 4/30\n","Train Loss: 0.2497 (KL: 0.0603, BCE: 0.4392)\n","Val Loss: 0.2555 (KL: 0.0612, BCE: 0.4499)\n","Current LR: 3.00e-04\n","Saved best student model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|██████████| 2544/2544 [07:28<00:00,  5.67it/s, loss=0.2465, kl=0.0612, bce=0.4319]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 5/30\n","Train Loss: 0.2465 (KL: 0.0612, BCE: 0.4319)\n","Val Loss: 0.2570 (KL: 0.0575, BCE: 0.4566)\n","Current LR: 3.00e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 2544/2544 [07:29<00:00,  5.66it/s, loss=0.2427, kl=0.0622, bce=0.4231]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 6/30\n","Train Loss: 0.2427 (KL: 0.0622, BCE: 0.4231)\n","Val Loss: 0.2586 (KL: 0.0632, BCE: 0.4540)\n","Current LR: 3.00e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|██████████| 2544/2544 [07:34<00:00,  5.60it/s, loss=0.2385, kl=0.0634, bce=0.4136]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 7/30\n","Train Loss: 0.2385 (KL: 0.0634, BCE: 0.4136)\n","Val Loss: 0.2636 (KL: 0.0627, BCE: 0.4645)\n","Current LR: 3.00e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|██████████| 2544/2544 [07:34<00:00,  5.60it/s, loss=0.2345, kl=0.0646, bce=0.4045]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 8/30\n","Train Loss: 0.2345 (KL: 0.0646, BCE: 0.4045)\n","Val Loss: 0.2675 (KL: 0.0631, BCE: 0.4718)\n","Current LR: 1.50e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|██████████| 2544/2544 [07:36<00:00,  5.57it/s, loss=0.2259, kl=0.0671, bce=0.3846]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 9/30\n","Train Loss: 0.2259 (KL: 0.0671, BCE: 0.3846)\n","Val Loss: 0.2774 (KL: 0.0650, BCE: 0.4897)\n","Current LR: 1.50e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|██████████| 2544/2544 [07:30<00:00,  5.64it/s, loss=0.2210, kl=0.0686, bce=0.3734]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 10/30\n","Train Loss: 0.2210 (KL: 0.0686, BCE: 0.3734)\n","Val Loss: 0.2877 (KL: 0.0616, BCE: 0.5138)\n","Current LR: 1.50e-04\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11: 100%|██████████| 2544/2544 [07:35<00:00,  5.58it/s, loss=0.2179, kl=0.0696, bce=0.3661]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 11/30\n","Train Loss: 0.2179 (KL: 0.0696, BCE: 0.3661)\n","Val Loss: 0.2961 (KL: 0.0672, BCE: 0.5250)\n","Current LR: 1.50e-04\n","Early stopping after 11 epochs\n","\n","Evaluating student model on test set:\n","\n","=== Evaluation Results ===\n","Strict Accuracy: 0.0132\n","Mean Accuracy: 0.8977\n","Top-5 Accuracy: 0.9106\n","Top-10 Accuracy: 0.7392\n","Top-20 Accuracy: 0.4549\n","\n","Per-Attribute Metrics (Top 10 by F1 score):\n","-------------------------------------------------------------------------------------\n","Attribute                Accuracy  Precision Recall    F1        Support   \n","-------------------------------------------------------------------------------------\n","No_Beard                 0.9562    0.9579    0.9923    0.9748       17041.0\n","Eyeglasses               0.9966    0.9788    0.9682    0.9735        1289.0\n","Male                     0.9782    0.9599    0.9846    0.9721        7715.0\n","Wearing_Lipstick         0.9428    0.9209    0.9742    0.9468       10418.0\n","Mouth_Slightly_Open      0.9354    0.9345    0.9350    0.9348        9883.0\n","Smiling                  0.9268    0.9326    0.9202    0.9264        9987.0\n","Young                    0.8625    0.8554    0.9849    0.9156       15114.0\n","Heavy_Makeup             0.9133    0.8741    0.9181    0.8956        8084.0\n","Wearing_Hat              0.9904    0.8657    0.9142    0.8893         839.0\n","Bangs                    0.9589    0.8582    0.8820    0.8699        3109.0\n","\n","Saved final student model\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import models\n","from tqdm import tqdm\n","import numpy as np\n","\n","# Configuration\n","class Config:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    num_classes = 40\n","    batch_size = 128\n","    lr = 3e-4\n","    max_epochs = 30\n","    temperature = 1.0  # Start with simpler temperature\n","    alpha = 0.5  # Balanced weight between teacher and ground truth\n","    patience = 7\n","    grad_clip = 1.0\n","    grad_accum_steps = 2\n","    mean = [0.485, 0.456, 0.406]\n","    std = [0.229, 0.224, 0.225]\n","    attribute_names = [\n","        '5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n","        'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n","        'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n","        'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n","        'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n","        'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline', 'Rosy_Cheeks',\n","        'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings',\n","        'Wearing_Hat', 'Wearing_Lipstick', 'Wearing_Necklace', 'Wearing_Necktie', 'Young'\n","    ]\n","\n","# Teacher Model (ResNet50)\n","class TeacherModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n","\n","        # Freeze early layers\n","        for param in self.backbone.parameters():\n","            param.requires_grad = False\n","        for param in self.backbone.layer3.parameters():\n","            param.requires_grad = True\n","        for param in self.backbone.layer4.parameters():\n","            param.requires_grad = True\n","\n","        # Classifier head\n","        self.backbone.fc = nn.Sequential(\n","            nn.Dropout(0.3),\n","            nn.Linear(self.backbone.fc.in_features, 1024),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(1024, num_classes))\n","\n","    def forward(self, x):\n","        return self.backbone(x)\n","\n","# Student Model (ResNet18)\n","class StudentModel(nn.Module):\n","    def __init__(self, num_classes=40):\n","        super().__init__()\n","        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","        self.backbone.fc = nn.Sequential(\n","            nn.Dropout(0.2),\n","            nn.Linear(self.backbone.fc.in_features, 512),\n","            nn.ReLU(),\n","            nn.Dropout(0.1),\n","            nn.Linear(512, num_classes))\n","\n","    def forward(self, x):\n","        return self.backbone(x)\n","\n","# Proper Distillation Loss for Multi-Label Classification\n","class DistillationLoss(nn.Module):\n","    def __init__(self, temperature, alpha):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.alpha = alpha\n","        self.bce_loss = nn.BCEWithLogitsLoss()\n","\n","    def forward(self, student_logits, teacher_logits, targets):\n","        # Teacher probabilities with temperature\n","        teacher_probs = torch.sigmoid(teacher_logits / self.temperature)\n","        student_probs = torch.sigmoid(student_logits / self.temperature)\n","\n","        # BCE loss with original logits (not temperature-scaled)\n","        bce_loss = self.bce_loss(student_logits, targets)\n","\n","        # KL divergence between teacher and student probabilities\n","        kl_loss = (teacher_probs * (torch.log(teacher_probs + 1e-10) -\n","                  torch.log(student_probs + 1e-10))).mean()\n","\n","        # Combined loss\n","        total_loss = (self.alpha * kl_loss +\n","                     (1 - self.alpha) * bce_loss)\n","\n","        return total_loss, {'kl_loss': kl_loss.item(), 'bce_loss': bce_loss.item()}\n","\n","# Training Function\n","def train_with_distillation(student, teacher, train_loader, val_loader):\n","    optimizer = optim.AdamW(\n","        student.parameters(),\n","        lr=Config.lr,\n","        weight_decay=1e-4\n","    )\n","\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode='min',\n","        factor=0.5,\n","        patience=3,\n","        verbose=True\n","    )\n","\n","    criterion = DistillationLoss(Config.temperature, Config.alpha)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    best_val_loss = float('inf')\n","    patience_counter = 0\n","\n","    for epoch in range(Config.max_epochs):\n","        student.train()\n","        train_loss = 0.0\n","        loss_components = {'kl_loss': 0.0, 'bce_loss': 0.0}\n","\n","        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n","\n","        for step, (images, labels) in enumerate(pbar):\n","            images = images.to(Config.device, non_blocking=True)\n","            labels = labels.to(Config.device, non_blocking=True)\n","\n","            with torch.cuda.amp.autocast():\n","                student_logits = student(images)\n","                with torch.no_grad():\n","                    teacher_logits = teacher(images)\n","                loss, comp = criterion(student_logits, teacher_logits, labels)\n","                loss = loss / Config.grad_accum_steps\n","\n","            scaler.scale(loss).backward()\n","\n","            if (step + 1) % Config.grad_accum_steps == 0:\n","                scaler.unscale_(optimizer)\n","                torch.nn.utils.clip_grad_norm_(student.parameters(), Config.grad_clip)\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad(set_to_none=True)\n","\n","            train_loss += loss.item() * Config.grad_accum_steps\n","            for k in loss_components:\n","                loss_components[k] += comp[k]\n","            pbar.set_postfix({\n","                'loss': f\"{train_loss/(step+1):.4f}\",\n","                'kl': f\"{loss_components['kl_loss']/(step+1):.4f}\",\n","                'bce': f\"{loss_components['bce_loss']/(step+1):.4f}\"\n","            })\n","\n","        # Validation\n","        val_loss = 0.0\n","        val_components = {'kl_loss': 0.0, 'bce_loss': 0.0}\n","        student.eval()\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images = images.to(Config.device)\n","                labels = labels.to(Config.device)\n","\n","                with torch.cuda.amp.autocast():\n","                    student_logits = student(images)\n","                    teacher_logits = teacher(images)\n","                    loss, comp = criterion(student_logits, teacher_logits, labels)\n","                    val_loss += loss.item()\n","                    for k in val_components:\n","                        val_components[k] += comp[k]\n","\n","        avg_train_loss = train_loss / len(train_loader)\n","        avg_val_loss = val_loss / len(val_loader)\n","\n","        scheduler.step(avg_val_loss)\n","\n","        print(f\"\\nEpoch {epoch+1}/{Config.max_epochs}\")\n","        print(f\"Train Loss: {avg_train_loss:.4f} (KL: {loss_components['kl_loss']/len(train_loader):.4f}, BCE: {loss_components['bce_loss']/len(train_loader):.4f})\")\n","        print(f\"Val Loss: {avg_val_loss:.4f} (KL: {val_components['kl_loss']/len(val_loader):.4f}, BCE: {val_components['bce_loss']/len(val_loader):.4f})\")\n","        print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","        if avg_val_loss < best_val_loss:\n","            best_val_loss = avg_val_loss\n","            patience_counter = 0\n","            torch.save({\n","                'model': student.state_dict(),\n","                'optimizer': optimizer.state_dict(),\n","                'epoch': epoch,\n","                'best_val_loss': best_val_loss\n","            }, 'best_student_model.pth')\n","            print(\"Saved best student model\")\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= Config.patience:\n","                print(f\"Early stopping after {epoch+1} epochs\")\n","                break\n","\n","    checkpoint = torch.load('best_student_model.pth')\n","    student.load_state_dict(checkpoint['model'])\n","    return student\n","\n","# Evaluation Function\n","def evaluate_model(model, loader, attribute_names=None):\n","    model.eval()\n","    results = {\n","        'strict_acc': 0.0,\n","        'mean_acc': 0.0,\n","        'top_k_acc': {5: 0.0, 10: 0.0, 20: 0.0},\n","        'per_attribute_acc': np.zeros(Config.num_classes),\n","        'confusion_matrix': np.zeros((2, 2, Config.num_classes))\n","    }\n","\n","    total_samples = 0\n","\n","    with torch.no_grad():\n","        for images, labels in loader:\n","            images = images.to(Config.device)\n","            labels = labels.to(Config.device).float()\n","            batch_size = images.size(0)\n","\n","            outputs = model(images)\n","            probs = torch.sigmoid(outputs)\n","            preds = (probs > 0.5).float()\n","\n","            results['strict_acc'] += (preds == labels).all(dim=1).sum().item()\n","            results['mean_acc'] += (preds == labels).float().mean(dim=1).sum().item()\n","\n","            for k in results['top_k_acc'].keys():\n","                _, topk_indices = torch.topk(probs, k, dim=1)\n","                correct = torch.gather(labels, 1, topk_indices).sum(dim=1)\n","                results['top_k_acc'][k] += (correct.float() / k).sum().item()\n","\n","            for attr in range(Config.num_classes):\n","                attr_pred = preds[:, attr]\n","                attr_label = labels[:, attr]\n","                results['per_attribute_acc'][attr] += (attr_pred == attr_label).sum().item()\n","                results['confusion_matrix'][0, 0, attr] += ((attr_pred == 1) & (attr_label == 1)).sum().item()\n","                results['confusion_matrix'][0, 1, attr] += ((attr_pred == 1) & (attr_label == 0)).sum().item()\n","                results['confusion_matrix'][1, 0, attr] += ((attr_pred == 0) & (attr_label == 1)).sum().item()\n","                results['confusion_matrix'][1, 1, attr] += ((attr_pred == 0) & (attr_label == 0)).sum().item()\n","\n","            total_samples += batch_size\n","\n","    results['strict_acc'] /= total_samples\n","    results['mean_acc'] /= total_samples\n","    for k in results['top_k_acc']:\n","        results['top_k_acc'][k] /= total_samples\n","    results['per_attribute_acc'] /= total_samples\n","\n","    attribute_metrics = {}\n","    for attr in range(Config.num_classes):\n","        tp = results['confusion_matrix'][0, 0, attr]\n","        fp = results['confusion_matrix'][0, 1, attr]\n","        fn = results['confusion_matrix'][1, 0, attr]\n","        tn = results['confusion_matrix'][1, 1, attr]\n","\n","        precision = tp / (tp + fp + 1e-10)\n","        recall = tp / (tp + fn + 1e-10)\n","        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n","\n","        attr_name = attribute_names[attr] if attribute_names else f\"Attr {attr}\"\n","        attribute_metrics[attr_name] = {\n","            'accuracy': (tp + tn) / (tp + tn + fp + fn),\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'support': tp + fn\n","        }\n","\n","    print(\"\\n=== Evaluation Results ===\")\n","    print(f\"Strict Accuracy: {results['strict_acc']:.4f}\")\n","    print(f\"Mean Accuracy: {results['mean_acc']:.4f}\")\n","    for k, acc in sorted(results['top_k_acc'].items()):\n","        print(f\"Top-{k} Accuracy: {acc:.4f}\")\n","\n","    if attribute_names:\n","        print(\"\\nPer-Attribute Metrics (Top 10 by F1 score):\")\n","        print(\"-\" * 85)\n","        print(f\"{'Attribute':<25}{'Accuracy':<10}{'Precision':<10}{'Recall':<10}{'F1':<10}{'Support':<10}\")\n","        print(\"-\" * 85)\n","\n","        sorted_attrs = sorted(attribute_metrics.items(),\n","                            key=lambda x: x[1]['f1'],\n","                            reverse=True)[:10]\n","\n","        for name, metrics in sorted_attrs:\n","            print(f\"{name:<25}{metrics['accuracy']:.4f}    {metrics['precision']:.4f}    {metrics['recall']:.4f}    {metrics['f1']:.4f}    {metrics['support']:>10}\")\n","\n","    return results, attribute_metrics\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Initialize data loaders (you need to define these)\n","    # train_loader = ...\n","    # valid_loader = ...\n","    # test_loader = ...\n","\n","    # Load teacher model\n","    print(\"Loading teacher model...\")\n","    teacher = TeacherModel(num_classes=Config.num_classes)\n","    teacher_checkpoint = torch.load('C:/Users/akash/Downloads/Soft.pth', map_location=Config.device)\n","\n","    # Handle potential DataParallel wrapper\n","    if all(k.startswith('module.') for k in teacher_checkpoint.keys()):\n","        from collections import OrderedDict\n","        new_state_dict = OrderedDict()\n","        for k, v in teacher_checkpoint.items():\n","            name = k[7:]  # remove 'module.' prefix\n","            new_state_dict[name] = v\n","        teacher_checkpoint = new_state_dict\n","\n","    teacher.load_state_dict(teacher_checkpoint, strict=False)\n","    teacher = teacher.to(Config.device)\n","    teacher.eval()\n","\n","    # Create student model\n","    print(\"Initializing student model...\")\n","    student = StudentModel(num_classes=Config.num_classes)\n","    student = student.to(Config.device)\n","\n","    # Train with distillation\n","    print(\"\\nStarting knowledge distillation training...\")\n","    trained_student = train_with_distillation(student, teacher, train_loader, valid_loader)\n","\n","    # Evaluate on test set\n","    print(\"\\nEvaluating student model on test set:\")\n","    test_results, attribute_metrics = evaluate_model(trained_student, test_loader, Config.attribute_names)\n","\n","    # Save final student model\n","    torch.save(trained_student.state_dict(), 'final_student_model.pth')\n","    print(\"\\nSaved final student model\")"]},{"cell_type":"code","source":[],"metadata":{"id":"LAtevED9q_Ip"},"execution_count":null,"outputs":[]}]}