{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZVDLlT3P0u-9","outputId":"d9a945be-986b-48ad-c9d1-038097189781"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-60f70b1bf898>:28: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n","  self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n"]},{"output_type":"stream","name":"stdout","text":["Loaded batch: torch.Size([64, 3, 156, 128]), torch.Size([64, 40])\n","Total samples: 202599\n"]}],"source":["# Mount Drive, extract CelebA, and create DataLoaders in one cell\n","from google.colab import drive\n","import zipfile\n","import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","\n","# 1. Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# # 2. Extract CelebA ZIP (update your path)\n","zip_path = '/content/drive/MyDrive/img_align_celeba.zip'  # Change to your path\n","extract_path = '/content/celeba'\n","\n","# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","#     for file in tqdm(zip_ref.namelist(), desc='Extracting'):\n","#         zip_ref.extract(file, extract_path)\n","\n","# 3. Dataset Class (Loads All Data Without Splitting)\n","class CelebADataset(Dataset):\n","    def __init__(self, root_dir, transform=None):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        self.attributes = pd.read_csv(os.path.join(root_dir, 'list_attr_celeba.txt'),\n","                                      delim_whitespace=True, header=1)\n","        self.filenames = self.attributes.index.tolist()  # Load all images\n","        self.attributes = (self.attributes + 1) // 2  # Convert -1/1 to 0/1\n","\n","    def __len__(self):\n","        return len(self.filenames)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.root_dir, 'img_align_celeba', self.filenames[idx])\n","        try:\n","            img = Image.open(img_path).convert('RGB')\n","        except Exception as e:\n","            print(f\"Error loading image {img_path}: {e}\")\n","            img = torch.zeros(3, 128, 128)  # Return a blank image on failure\n","\n","        attrs = self.attributes.iloc[idx].values.astype('float32')\n","        if self.transform:\n","            img = self.transform(img)\n","        return img, torch.from_numpy(attrs)\n","\n","# 4. Create DataLoader for Entire Dataset\n","transform = transforms.Compose([\n","    transforms.Resize(128),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5]*3, [0.5]*3)\n","])\n","\n","full_loader = DataLoader(\n","    CelebADataset(extract_path, transform),\n","    batch_size=64, shuffle=True\n",")\n","\n","# Verify\n","images, attrs = next(iter(full_loader))\n","print(f\"Loaded batch: {images.shape}, {attrs.shape}\")\n","print(f\"Total samples: {len(full_loader.dataset)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"naAgI_r2E6dY","outputId":"77939055-9f98-4bcf-ecb7-41f7df44f6ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of images found: 202599\n"]}],"source":["import os\n","\n","num_images = len(os.listdir('/content/celeba/img_align_celeba'))\n","print(f\"Number of images found: {num_images}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kAlHfx2m8bPw"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tv3Lokv7talB","outputId":"13f60ccc-2b1a-4e6b-c63f-29af7cc9e106"},"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n","100%|██████████| 97.8M/97.8M [00:01<00:00, 88.1MB/s]\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 71.0MB/s]\n","Epoch 1: 100%|██████████| 3166/3166 [08:26<00:00,  6.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 | Train Loss: 0.6188\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2: 100%|██████████| 3166/3166 [08:23<00:00,  6.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 | Train Loss: 0.6155\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3: 100%|██████████| 3166/3166 [08:22<00:00,  6.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 | Train Loss: 0.6141\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4: 100%|██████████| 3166/3166 [08:23<00:00,  6.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 | Train Loss: 0.6128\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5: 100%|██████████| 3166/3166 [08:22<00:00,  6.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 | Train Loss: 0.6111\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6: 100%|██████████| 3166/3166 [08:24<00:00,  6.28it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 | Train Loss: 0.6092\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7: 100%|██████████| 3166/3166 [08:23<00:00,  6.29it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 | Train Loss: 0.6072\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8: 100%|██████████| 3166/3166 [08:24<00:00,  6.27it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 | Train Loss: 0.6051\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9: 100%|██████████| 3166/3166 [08:26<00:00,  6.25it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 | Train Loss: 0.6032\n","Best model saved!\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10: 100%|██████████| 3166/3166 [08:25<00:00,  6.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 | Train Loss: 0.6016\n","Best model saved!\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torchvision import models\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","\n","# Configuration\n","class Config:\n","    root_dir = '/content/celeba/img_align_celeba'  # Ensure this matches the first section\n","    csv_path = '/content/celeba/list_attr_celeba.txt'\n","    batch_size = 64\n","    lr = 1e-4\n","    temperature = 2.0\n","    alpha = 0.7\n","    num_classes = 40\n","    max_epochs = 10\n","    model_save_path = 'distilled_student.pth'\n","    pretrained_weights = models.ResNet50_Weights.IMAGENET1K_V2\n","\n","# Use the same DataLoader from previous cell (full_loader)\n","train_loader = full_loader  # Reuse the already loaded DataLoader\n","\n","# Load Teacher Model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","teacher = models.resnet50(weights=Config.pretrained_weights)\n","\n","# Modify FC layer to match expected output\n","teacher.fc = nn.Sequential(\n","    nn.Linear(teacher.fc.in_features, 512),\n","    nn.ReLU(),\n","    nn.Linear(512, Config.num_classes)\n",")\n","\n","# Load pre-trained weights\n","checkpoint = torch.load('/content/drive/MyDrive/best_model.pth', map_location=device)\n","teacher.load_state_dict(checkpoint, strict=False)\n","teacher = teacher.to(device).eval()\n","\n","# Load Student Model\n","student = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n","student.fc = nn.Linear(student.fc.in_features, Config.num_classes)\n","student = student.to(device).train()\n","\n","# Loss functions\n","def distillation_loss(student_logits, teacher_logits, temperature=Config.temperature):\n","    soft_teacher = torch.sigmoid(teacher_logits / temperature)\n","    soft_student = torch.sigmoid(student_logits / temperature)\n","    return F.binary_cross_entropy(soft_student, soft_teacher)\n","\n","def attribute_loss(student_logits, targets):\n","    return F.binary_cross_entropy_with_logits(student_logits, targets)\n","\n","# Optimizer\n","optimizer = optim.Adam(student.parameters(), lr=Config.lr)\n","\n","# Training loop\n","best_val_loss = float('inf')\n","for epoch in range(Config.max_epochs):\n","    student.train()\n","    total_loss = 0\n","    for images, attrs in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n","        images, attrs = images.to(device), attrs.to(device)\n","\n","        with torch.no_grad():\n","            teacher_logits = teacher(images)\n","\n","        student_logits = student(images)\n","        loss = (Config.alpha * distillation_loss(student_logits, teacher_logits) +\n","                (1 - Config.alpha) * attribute_loss(student_logits, attrs))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n","\n","    # Save best model\n","    if avg_loss < best_val_loss:\n","        best_val_loss = avg_loss\n","        torch.save(student.state_dict(), Config.model_save_path)\n","        print(\"Best model saved!\")\n"]},{"cell_type":"code","source":["# Function to calculate accuracy\n","def accuracy(predictions, targets):\n","    # Convert logits to binary predictions using sigmoid\n","    preds = torch.sigmoid(predictions)\n","    preds = (preds > 0.5).float()  # Convert to binary (0 or 1)\n","    correct = (preds == targets).float()  # Check if predictions match targets\n","    return correct.sum() / correct.numel()  # Compute the accuracy\n","\n","# Function to evaluate a model (either Teacher or Student)\n","def evaluate_model(model, data_loader, device):\n","    model.eval()  # Set the model to evaluation mode\n","    total_correct = 0\n","    total_samples = 0\n","    with torch.no_grad():  # Disable gradient calculation during evaluation\n","        for images, attrs in tqdm(data_loader, desc=f'Evaluating {model.__class__.__name__}'):\n","            images, attrs = images.to(device), attrs.to(device)\n","\n","            # Forward pass through the model\n","            logits = model(images)\n","\n","            # Calculate accuracy\n","            total_correct += accuracy(logits, attrs) * attrs.size(0)\n","            total_samples += attrs.size(0)\n","\n","    # Final accuracy\n","    accuracy_value = total_correct / total_samples\n","    return accuracy_value\n","\n","# Test Teacher Model on Full Dataset\n","teacher_accuracy = evaluate_model(teacher, full_loader, device)\n","print(f\"Teacher Model Accuracy: {teacher_accuracy:.4f}\")\n","\n","# Test Student Model on Full Dataset\n","student_accuracy = evaluate_model(student, full_loader, device)\n","print(f\"Student Model Accuracy: {student_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5PT7r0k4AFNy","outputId":"611d2e88-12a9-42f7-9b40-5f3e44d2aff9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Evaluating ResNet: 100%|██████████| 3166/3166 [05:29<00:00,  9.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Teacher Model Accuracy: 0.5224\n"]},{"output_type":"stream","name":"stderr","text":["Evaluating ResNet: 100%|██████████| 3166/3166 [05:10<00:00, 10.19it/s]"]},{"output_type":"stream","name":"stdout","text":["Student Model Accuracy: 0.9816\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}